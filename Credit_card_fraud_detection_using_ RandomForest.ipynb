{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf031628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d459b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset \n",
    "dataset = pd.read_csv(\"C:/Users/91745/Downloads/archive_creditcard/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03880c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9c06e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75eba1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null values\n",
    "dataset.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fca07e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check balanced or not\n",
    "dataset['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "613c4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fraud: (284315, 31)\n",
      "Shape of nonfraud: (492, 31)\n"
     ]
    }
   ],
   "source": [
    "# balancing the dataset\n",
    "fraud_credit_data = dataset[dataset['Class']==0]\n",
    "nonfraud_credit_data = dataset[dataset['Class']==1]\n",
    "\n",
    "print(\"Shape of fraud:\",fraud_credit_data.shape)\n",
    "print(\"Shape of nonfraud:\",nonfraud_credit_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d38c67cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541    406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623    472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "541  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "623   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "4920  0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726   \n",
       "6108 -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207   \n",
       "6329  1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "541   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4920 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6108 -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6329 -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonfraud_credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e543f14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_sample = fraud_credit_data.sample(n=500)\n",
    "fraud_sample.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4a98a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 31)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset=pd.concat([fraud_sample,nonfraud_credit_data],axis=0)\n",
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d5fba1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create feautures and labels and train test split\n",
    "X=new_dataset.drop(columns=['Class'],axis=1)\n",
    "Y=new_dataset['Class']\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27f72ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "643e6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcef4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[LogisticRegression(max_iter=1000000),LinearSVC(),RandomForestClassifier(),MLPClassifier(),GaussianNB(),DecisionTreeClassifier()]\n",
    "\n",
    "def classifier_model():\n",
    "    for model in model_list:\n",
    "        model.fit(X_train,Y_train)\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        acc_score_train = accuracy_score(Y_train,y_train_pred)\n",
    "        acc_score_test = accuracy_score(Y_test,y_test_pred)\n",
    "        print(\"Accuracy train :\",model,\" is: \",round(acc_score_train *100,2),\" %\")\n",
    "        print(\"Accuracy test :\",model,\" is: \",round(acc_score_test*100,2),\" %\")\n",
    "        print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d947a865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train : LogisticRegression(max_iter=1000000)  is:  94.83  %\n",
      "Accuracy test : LogisticRegression(max_iter=1000000)  is:  93.97  %\n",
      "**************\n",
      "Accuracy train : LinearSVC()  is:  94.45  %\n",
      "Accuracy test : LinearSVC()  is:  95.48  %\n",
      "**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train : RandomForestClassifier()  is:  100.0  %\n",
      "Accuracy test : RandomForestClassifier()  is:  94.47  %\n",
      "**************\n",
      "Accuracy train : MLPClassifier()  is:  98.36  %\n",
      "Accuracy test : MLPClassifier()  is:  94.97  %\n",
      "**************\n",
      "Accuracy train : GaussianNB()  is:  90.79  %\n",
      "Accuracy test : GaussianNB()  is:  90.95  %\n",
      "**************\n",
      "Accuracy train : DecisionTreeClassifier()  is:  100.0  %\n",
      "Accuracy test : DecisionTreeClassifier()  is:  91.46  %\n",
      "**************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f9e0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation for accuracy checking\n",
    "model_list=[LogisticRegression(max_iter=1000000),LinearSVC(),RandomForestClassifier(),MLPClassifier(),GaussianNB(),DecisionTreeClassifier()]\n",
    "\n",
    "def classifier_model_with_cv():\n",
    "\n",
    "    for model in model_list:\n",
    "        cv_score = cross_val_score(model,X,Y,cv=5)\n",
    "        mean_score=sum(cv_score)/len(cv_score)\n",
    "        mean_score=mean_score*100\n",
    "        mean_score=round(mean_score,2)\n",
    "\n",
    "\n",
    "        print(\"Accuracy test :\",model,\" is: \",mean_score)\n",
    "        print(\"*******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f638b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test : LogisticRegression(max_iter=1000000)  is:  92.54\n",
      "*******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test : LinearSVC()  is:  61.79\n",
      "*******\n",
      "Accuracy test : RandomForestClassifier()  is:  92.54\n",
      "*******\n",
      "Accuracy test : MLPClassifier()  is:  49.6\n",
      "*******\n",
      "Accuracy test : GaussianNB()  is:  84.67\n",
      "*******\n",
      "Accuracy test : DecisionTreeClassifier()  is:  90.72\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "classifier_model_with_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26050ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.1581347 , 0.84624338, 1.64474115, 0.15918627, 0.77198186,\n",
      "       1.63458595, 0.16082845, 0.77074652, 1.54187608]), 'std_fit_time': array([0.01821821, 0.04110863, 0.06166031, 0.00618273, 0.03134182,\n",
      "       0.1051349 , 0.0117498 , 0.04157818, 0.0577811 ]), 'mean_score_time': array([0.00765905, 0.01645141, 0.02974553, 0.00766706, 0.01990886,\n",
      "       0.01860461, 0.00336461, 0.01752481, 0.03382425]), 'std_score_time': array([0.00699182, 0.00115396, 0.00534613, 0.00110468, 0.00401078,\n",
      "       0.00512449, 0.00412122, 0.00380666, 0.00372718]), 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'entropy', 'entropy',\n",
      "                   'entropy', 'log_loss', 'log_loss', 'log_loss'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[10, 50, 100, 10, 50, 100, 10, 50, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'criterion': 'gini', 'n_estimators': 10}, {'criterion': 'gini', 'n_estimators': 50}, {'criterion': 'gini', 'n_estimators': 100}, {'criterion': 'entropy', 'n_estimators': 10}, {'criterion': 'entropy', 'n_estimators': 50}, {'criterion': 'entropy', 'n_estimators': 100}, {'criterion': 'log_loss', 'n_estimators': 10}, {'criterion': 'log_loss', 'n_estimators': 50}, {'criterion': 'log_loss', 'n_estimators': 100}], 'split0_test_score': array([0.93467337, 0.94472362, 0.94974874, 0.95477387, 0.93467337,\n",
      "       0.93467337, 0.94472362, 0.94974874, 0.94974874]), 'split1_test_score': array([0.91457286, 0.93969849, 0.92964824, 0.92462312, 0.92964824,\n",
      "       0.91959799, 0.92462312, 0.91457286, 0.93467337]), 'split2_test_score': array([0.9040404 , 0.92424242, 0.91919192, 0.91919192, 0.91414141,\n",
      "       0.9040404 , 0.9040404 , 0.91414141, 0.91919192]), 'split3_test_score': array([0.93434343, 0.96464646, 0.93939394, 0.94444444, 0.94949495,\n",
      "       0.93939394, 0.94949495, 0.94444444, 0.94444444]), 'split4_test_score': array([0.9040404 , 0.91919192, 0.91919192, 0.91414141, 0.8989899 ,\n",
      "       0.91414141, 0.9040404 , 0.90909091, 0.9040404 ]), 'mean_test_score': array([0.91833409, 0.93850058, 0.93143495, 0.93143495, 0.92538957,\n",
      "       0.92236942, 0.9253845 , 0.92639968, 0.93041978]), 'std_test_score': array([0.01375527, 0.01612382, 0.01184666, 0.01555192, 0.01737158,\n",
      "       0.01305799, 0.01932375, 0.01709108, 0.01679482]), 'rank_test_score': array([9, 1, 2, 2, 6, 8, 7, 5, 4])}\n",
      "0.9385005837267144\n"
     ]
    }
   ],
   "source": [
    "# hyperparamter tuning\n",
    "parameters={\n",
    "    'n_estimators':[10,50,100],\n",
    "    'criterion':[\"gini\", \"entropy\", \"log_loss\"]\n",
    "\n",
    "}\n",
    "modelX=RandomForestClassifier()\n",
    "classifier=  GridSearchCV(modelX,parameters,cv=5)\n",
    "classifier.fit(X,Y)\n",
    "print(classifier.cv_results_)\n",
    "print(classifier.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final classifier selected - RandomFoerest Classifier - It gave 93.85% accuracy on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bace3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(classifier.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22139ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train : RandomForestClassifier(n_estimators=50)  is:  99.87  %\n",
      "Accuracy test : RandomForestClassifier(n_estimators=50)  is:  93.47  %\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "# finally the model selected - Random Forest with paramters are :- n_estimators=50 and criterion=\"gini\" \n",
    "\n",
    "final_model=RandomForestClassifier(criterion=\"gini\",n_estimators=50)\n",
    "final_model.fit(X_train,Y_train)\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "acc_score_train = accuracy_score(Y_train,y_train_pred)\n",
    "acc_score_test = accuracy_score(Y_test,y_test_pred)\n",
    "print(\"Accuracy train :\",final_model,\" is: \",round(acc_score_train *100,2),\" %\")\n",
    "print(\"Accuracy test :\",final_model,\" is: \",round(acc_score_test*100,2),\" %\")\n",
    "print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99894825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query point\n",
    "input_data2=(1,-1.35835406159823,-1.34016307473609,1.77320934263119,0.379779593034328,-0.503198133318193,1.80049938079263,0.791460956450422,0.247675786588991,-1.51465432260583,0.207642865216696,0.624501459424895,0.066083685268831,0.717292731410831,-0.165945922763554,2.34586494901581,-2.89008319444231,1.10996937869599,-0.121359313195888,-2.26185709530414,0.524979725224404,0.247998153469754,0.771679401917229,0.909412262347719,-0.689280956490685,-0.327641833735251,-0.139096571514147,-0.0553527940384261,-0.0597518405929204,378.66,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2048879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00 -1.35835406e+00 -1.34016307e+00  1.77320934e+00\n",
      "  3.79779593e-01 -5.03198133e-01  1.80049938e+00  7.91460956e-01\n",
      "  2.47675787e-01 -1.51465432e+00  2.07642865e-01  6.24501459e-01\n",
      "  6.60836853e-02  7.17292731e-01 -1.65945923e-01  2.34586495e+00\n",
      " -2.89008319e+00  1.10996938e+00 -1.21359313e-01 -2.26185710e+00\n",
      "  5.24979725e-01  2.47998153e-01  7.71679402e-01  9.09412262e-01\n",
      " -6.89280956e-01 -3.27641834e-01 -1.39096572e-01 -5.53527940e-02\n",
      " -5.97518406e-02  3.78660000e+02]\n",
      "[[ 1.00000000e+00 -1.35835406e+00 -1.34016307e+00  1.77320934e+00\n",
      "   3.79779593e-01 -5.03198133e-01  1.80049938e+00  7.91460956e-01\n",
      "   2.47675787e-01 -1.51465432e+00  2.07642865e-01  6.24501459e-01\n",
      "   6.60836853e-02  7.17292731e-01 -1.65945923e-01  2.34586495e+00\n",
      "  -2.89008319e+00  1.10996938e+00 -1.21359313e-01 -2.26185710e+00\n",
      "   5.24979725e-01  2.47998153e-01  7.71679402e-01  9.09412262e-01\n",
      "  -6.89280956e-01 -3.27641834e-01 -1.39096572e-01 -5.53527940e-02\n",
      "  -5.97518406e-02  3.78660000e+02]]\n"
     ]
    }
   ],
   "source": [
    "input_array = np.asarray(input_data2)\n",
    "print(input_array)\n",
    "input_array=input_array.reshape(1,-1)\n",
    "print(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ac0c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is Safe!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Output for the query point\n",
    "input_df=pd.DataFrame(input_array.reshape(1,-1))\n",
    "# print(input_df)\n",
    "y_predd = final_model.predict(input_df)\n",
    "# print(y_predd)\n",
    "if(y_predd==1):\n",
    "    print(\"It is Fraud X\")\n",
    "else:\n",
    "    print(\"It is Safe!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447818c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Deep Learning Neural Network efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cf0606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout,Flatten\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fbc02b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 16)                496       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 785 (3.07 KB)\n",
      "Trainable params: 785 (3.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating a model on some Hidden layers with relu as activation function and output with sigmoid,\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(16,activation='relu',input_shape=(30,)))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "795775c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile with optimizer =\"adam\"\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "363fa906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "23/23 [==============================] - 2s 20ms/step - loss: 0.6313 - accuracy: 0.7574 - val_loss: 0.6096 - val_accuracy: 0.7625\n",
      "Epoch 2/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5499 - accuracy: 0.7938 - val_loss: 0.5429 - val_accuracy: 0.7625\n",
      "Epoch 3/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4604 - accuracy: 0.8485 - val_loss: 0.4563 - val_accuracy: 0.8250\n",
      "Epoch 4/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3711 - accuracy: 0.8878 - val_loss: 0.3944 - val_accuracy: 0.8375\n",
      "Epoch 5/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3098 - accuracy: 0.9018 - val_loss: 0.3530 - val_accuracy: 0.8375\n",
      "Epoch 6/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2678 - accuracy: 0.9102 - val_loss: 0.3272 - val_accuracy: 0.8500\n",
      "Epoch 7/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2367 - accuracy: 0.9187 - val_loss: 0.2976 - val_accuracy: 0.8750\n",
      "Epoch 8/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2116 - accuracy: 0.9355 - val_loss: 0.2842 - val_accuracy: 0.8875\n",
      "Epoch 9/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1915 - accuracy: 0.9369 - val_loss: 0.2638 - val_accuracy: 0.9000\n",
      "Epoch 10/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1736 - accuracy: 0.9439 - val_loss: 0.2486 - val_accuracy: 0.9000\n",
      "Epoch 11/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9453 - val_loss: 0.2383 - val_accuracy: 0.9125\n",
      "Epoch 12/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.9495 - val_loss: 0.2293 - val_accuracy: 0.9125\n",
      "Epoch 13/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1444 - accuracy: 0.9523 - val_loss: 0.2217 - val_accuracy: 0.9250\n",
      "Epoch 14/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9537 - val_loss: 0.2114 - val_accuracy: 0.9250\n",
      "Epoch 15/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1273 - accuracy: 0.9593 - val_loss: 0.2061 - val_accuracy: 0.9250\n",
      "Epoch 16/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1212 - accuracy: 0.9635 - val_loss: 0.1937 - val_accuracy: 0.9250\n",
      "Epoch 17/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1168 - accuracy: 0.9649 - val_loss: 0.1920 - val_accuracy: 0.9250\n",
      "Epoch 18/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9663 - val_loss: 0.1893 - val_accuracy: 0.9250\n",
      "Epoch 19/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1076 - accuracy: 0.9691 - val_loss: 0.1908 - val_accuracy: 0.9250\n",
      "Epoch 20/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1031 - accuracy: 0.9677 - val_loss: 0.1720 - val_accuracy: 0.9250\n",
      "Epoch 21/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9635 - val_loss: 0.1963 - val_accuracy: 0.9250\n",
      "Epoch 22/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1016 - accuracy: 0.9635 - val_loss: 0.1691 - val_accuracy: 0.9125\n",
      "Epoch 23/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0959 - accuracy: 0.9705 - val_loss: 0.1734 - val_accuracy: 0.9250\n",
      "Epoch 24/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0924 - accuracy: 0.9748 - val_loss: 0.1613 - val_accuracy: 0.9250\n",
      "Epoch 25/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0903 - accuracy: 0.9734 - val_loss: 0.1625 - val_accuracy: 0.9250\n",
      "Epoch 26/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0880 - accuracy: 0.9734 - val_loss: 0.1558 - val_accuracy: 0.9250\n",
      "Epoch 27/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9748 - val_loss: 0.1521 - val_accuracy: 0.9250\n",
      "Epoch 28/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0856 - accuracy: 0.9776 - val_loss: 0.1629 - val_accuracy: 0.9250\n",
      "Epoch 29/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9762 - val_loss: 0.1527 - val_accuracy: 0.9250\n",
      "Epoch 30/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0821 - accuracy: 0.9748 - val_loss: 0.1414 - val_accuracy: 0.9250\n",
      "Epoch 31/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9776 - val_loss: 0.1417 - val_accuracy: 0.9250\n",
      "Epoch 32/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0787 - accuracy: 0.9776 - val_loss: 0.1453 - val_accuracy: 0.9250\n",
      "Epoch 33/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0768 - accuracy: 0.9776 - val_loss: 0.1388 - val_accuracy: 0.9500\n",
      "Epoch 34/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9748 - val_loss: 0.1523 - val_accuracy: 0.9375\n",
      "Epoch 35/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9790 - val_loss: 0.1346 - val_accuracy: 0.9500\n",
      "Epoch 36/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9790 - val_loss: 0.1416 - val_accuracy: 0.9250\n",
      "Epoch 37/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.9818 - val_loss: 0.1349 - val_accuracy: 0.9375\n",
      "Epoch 38/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9818 - val_loss: 0.1312 - val_accuracy: 0.9375\n",
      "Epoch 39/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0694 - accuracy: 0.9790 - val_loss: 0.1358 - val_accuracy: 0.9500\n",
      "Epoch 40/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9818 - val_loss: 0.1222 - val_accuracy: 0.9625\n",
      "Epoch 41/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9734 - val_loss: 0.1285 - val_accuracy: 0.9625\n",
      "Epoch 42/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9790 - val_loss: 0.1263 - val_accuracy: 0.9500\n",
      "Epoch 43/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9818 - val_loss: 0.1283 - val_accuracy: 0.9625\n",
      "Epoch 44/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0626 - accuracy: 0.9790 - val_loss: 0.1215 - val_accuracy: 0.9750\n",
      "Epoch 45/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9832 - val_loss: 0.1184 - val_accuracy: 0.9750\n",
      "Epoch 46/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9790 - val_loss: 0.1230 - val_accuracy: 0.9750\n",
      "Epoch 47/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9790 - val_loss: 0.1153 - val_accuracy: 0.9750\n",
      "Epoch 48/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0571 - accuracy: 0.9832 - val_loss: 0.1190 - val_accuracy: 0.9750\n",
      "Epoch 49/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0567 - accuracy: 0.9832 - val_loss: 0.1131 - val_accuracy: 0.9750\n",
      "Epoch 50/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0555 - accuracy: 0.9804 - val_loss: 0.1114 - val_accuracy: 0.9750\n",
      "Epoch 51/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9846 - val_loss: 0.1103 - val_accuracy: 0.9750\n",
      "Epoch 52/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 0.9874 - val_loss: 0.1103 - val_accuracy: 0.9750\n",
      "Epoch 53/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 0.9804 - val_loss: 0.1057 - val_accuracy: 0.9625\n",
      "Epoch 54/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.9860 - val_loss: 0.1095 - val_accuracy: 0.9750\n",
      "Epoch 55/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0506 - accuracy: 0.9818 - val_loss: 0.1082 - val_accuracy: 0.9750\n",
      "Epoch 56/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9832 - val_loss: 0.1058 - val_accuracy: 0.9750\n",
      "Epoch 57/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0470 - accuracy: 0.9874 - val_loss: 0.1055 - val_accuracy: 0.9750\n",
      "Epoch 58/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0468 - accuracy: 0.9888 - val_loss: 0.1040 - val_accuracy: 0.9750\n",
      "Epoch 59/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0453 - accuracy: 0.9832 - val_loss: 0.1068 - val_accuracy: 0.9750\n",
      "Epoch 60/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9832 - val_loss: 0.1094 - val_accuracy: 0.9750\n",
      "Epoch 61/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.9902 - val_loss: 0.1208 - val_accuracy: 0.9750\n",
      "Epoch 62/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0441 - accuracy: 0.9930 - val_loss: 0.1188 - val_accuracy: 0.9625\n",
      "Epoch 63/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 0.9902 - val_loss: 0.1150 - val_accuracy: 0.9750\n",
      "Epoch 64/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.1025 - val_accuracy: 0.9625\n",
      "Epoch 65/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0411 - accuracy: 0.9930 - val_loss: 0.1074 - val_accuracy: 0.9625\n",
      "Epoch 66/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9888 - val_loss: 0.1062 - val_accuracy: 0.9625\n",
      "Epoch 67/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9916 - val_loss: 0.1086 - val_accuracy: 0.9750\n",
      "Epoch 68/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9930 - val_loss: 0.1063 - val_accuracy: 0.9625\n",
      "Epoch 69/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.9930 - val_loss: 0.1159 - val_accuracy: 0.9625\n",
      "Epoch 70/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.9902 - val_loss: 0.1062 - val_accuracy: 0.9625\n",
      "Epoch 71/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0371 - accuracy: 0.9930 - val_loss: 0.0999 - val_accuracy: 0.9625\n",
      "Epoch 72/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0364 - accuracy: 0.9930 - val_loss: 0.1047 - val_accuracy: 0.9750\n",
      "Epoch 73/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0344 - accuracy: 0.9930 - val_loss: 0.1032 - val_accuracy: 0.9625\n",
      "Epoch 74/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0341 - accuracy: 0.9944 - val_loss: 0.1039 - val_accuracy: 0.9625\n",
      "Epoch 75/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0334 - accuracy: 0.9930 - val_loss: 0.1015 - val_accuracy: 0.9625\n",
      "Epoch 76/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 0.9916 - val_loss: 0.1153 - val_accuracy: 0.9625\n",
      "Epoch 77/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.9944 - val_loss: 0.1063 - val_accuracy: 0.9625\n",
      "Epoch 78/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9916 - val_loss: 0.1088 - val_accuracy: 0.9625\n",
      "Epoch 79/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0347 - accuracy: 0.9930 - val_loss: 0.1023 - val_accuracy: 0.9500\n",
      "Epoch 80/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0332 - accuracy: 0.9902 - val_loss: 0.1101 - val_accuracy: 0.9625\n",
      "Epoch 81/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0292 - accuracy: 0.9958 - val_loss: 0.1162 - val_accuracy: 0.9625\n",
      "Epoch 82/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0284 - accuracy: 0.9944 - val_loss: 0.1110 - val_accuracy: 0.9625\n",
      "Epoch 83/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0285 - accuracy: 0.9958 - val_loss: 0.1290 - val_accuracy: 0.9625\n",
      "Epoch 84/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 0.9944 - val_loss: 0.1104 - val_accuracy: 0.9500\n",
      "Epoch 85/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0262 - accuracy: 0.9958 - val_loss: 0.1218 - val_accuracy: 0.9625\n",
      "Epoch 86/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0262 - accuracy: 0.9958 - val_loss: 0.1188 - val_accuracy: 0.9625\n",
      "Epoch 87/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 0.9958 - val_loss: 0.1201 - val_accuracy: 0.9625\n",
      "Epoch 88/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0254 - accuracy: 0.9958 - val_loss: 0.1230 - val_accuracy: 0.9625\n",
      "Epoch 89/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0250 - accuracy: 0.9958 - val_loss: 0.1212 - val_accuracy: 0.9625\n",
      "Epoch 90/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0245 - accuracy: 0.9958 - val_loss: 0.1272 - val_accuracy: 0.9625\n",
      "Epoch 91/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0246 - accuracy: 0.9958 - val_loss: 0.1127 - val_accuracy: 0.9625\n",
      "Epoch 92/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0256 - accuracy: 0.9944 - val_loss: 0.1361 - val_accuracy: 0.9750\n",
      "Epoch 93/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0238 - accuracy: 0.9958 - val_loss: 0.1275 - val_accuracy: 0.9625\n",
      "Epoch 94/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0230 - accuracy: 0.9958 - val_loss: 0.1203 - val_accuracy: 0.9625\n",
      "Epoch 95/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0228 - accuracy: 0.9958 - val_loss: 0.1268 - val_accuracy: 0.9625\n",
      "Epoch 96/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0252 - accuracy: 0.9958 - val_loss: 0.1374 - val_accuracy: 0.9750\n",
      "Epoch 97/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0222 - accuracy: 0.9958 - val_loss: 0.1288 - val_accuracy: 0.9625\n",
      "Epoch 98/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0215 - accuracy: 0.9958 - val_loss: 0.1315 - val_accuracy: 0.9625\n",
      "Epoch 99/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.1312 - val_accuracy: 0.9625\n",
      "Epoch 100/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0198 - accuracy: 0.9958 - val_loss: 0.1378 - val_accuracy: 0.9625\n",
      "Epoch 101/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0204 - accuracy: 0.9958 - val_loss: 0.1298 - val_accuracy: 0.9625\n",
      "Epoch 102/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.1372 - val_accuracy: 0.9625\n",
      "Epoch 103/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0191 - accuracy: 0.9972 - val_loss: 0.1345 - val_accuracy: 0.9625\n",
      "Epoch 104/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.1387 - val_accuracy: 0.9625\n",
      "Epoch 105/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0196 - accuracy: 0.9986 - val_loss: 0.1527 - val_accuracy: 0.9750\n",
      "Epoch 106/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0197 - accuracy: 0.9958 - val_loss: 0.1405 - val_accuracy: 0.9625\n",
      "Epoch 107/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0181 - accuracy: 0.9986 - val_loss: 0.1470 - val_accuracy: 0.9750\n",
      "Epoch 108/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0183 - accuracy: 0.9972 - val_loss: 0.1370 - val_accuracy: 0.9625\n",
      "Epoch 109/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0169 - accuracy: 0.9972 - val_loss: 0.1389 - val_accuracy: 0.9625\n",
      "Epoch 110/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0170 - accuracy: 0.9958 - val_loss: 0.1375 - val_accuracy: 0.9500\n",
      "Epoch 111/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0176 - accuracy: 0.9986 - val_loss: 0.1447 - val_accuracy: 0.9625\n",
      "Epoch 112/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0166 - accuracy: 0.9972 - val_loss: 0.1473 - val_accuracy: 0.9625\n",
      "Epoch 113/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0177 - accuracy: 0.9972 - val_loss: 0.1410 - val_accuracy: 0.9500\n",
      "Epoch 114/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0157 - accuracy: 0.9986 - val_loss: 0.1575 - val_accuracy: 0.9750\n",
      "Epoch 115/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0167 - accuracy: 0.9986 - val_loss: 0.1464 - val_accuracy: 0.9500\n",
      "Epoch 116/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0158 - accuracy: 0.9972 - val_loss: 0.1432 - val_accuracy: 0.9500\n",
      "Epoch 117/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9986 - val_loss: 0.1497 - val_accuracy: 0.9500\n",
      "Epoch 118/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9972 - val_loss: 0.1470 - val_accuracy: 0.9500\n",
      "Epoch 119/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 0.9986 - val_loss: 0.1652 - val_accuracy: 0.9625\n",
      "Epoch 120/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.1514 - val_accuracy: 0.9500\n",
      "Epoch 121/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 0.9972 - val_loss: 0.1609 - val_accuracy: 0.9500\n",
      "Epoch 122/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0162 - accuracy: 0.9972 - val_loss: 0.1519 - val_accuracy: 0.9750\n",
      "Epoch 123/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.9986 - val_loss: 0.1536 - val_accuracy: 0.9625\n",
      "Epoch 124/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 0.1492 - val_accuracy: 0.9750\n",
      "Epoch 125/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.9972 - val_loss: 0.1505 - val_accuracy: 0.9625\n",
      "Epoch 126/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9986 - val_loss: 0.1546 - val_accuracy: 0.9500\n",
      "Epoch 127/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 0.9972 - val_loss: 0.1523 - val_accuracy: 0.9625\n",
      "Epoch 128/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 0.9986 - val_loss: 0.1555 - val_accuracy: 0.9625\n",
      "Epoch 129/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9986 - val_loss: 0.1532 - val_accuracy: 0.9750\n",
      "Epoch 130/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.9986 - val_loss: 0.1548 - val_accuracy: 0.9625\n",
      "Epoch 131/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 0.9986 - val_loss: 0.1623 - val_accuracy: 0.9750\n",
      "Epoch 132/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.1655 - val_accuracy: 0.9500\n",
      "Epoch 133/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9986 - val_loss: 0.1551 - val_accuracy: 0.9750\n",
      "Epoch 134/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.9986 - val_loss: 0.1626 - val_accuracy: 0.9625\n",
      "Epoch 135/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.9986 - val_loss: 0.1447 - val_accuracy: 0.9750\n",
      "Epoch 136/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.9986 - val_loss: 0.1529 - val_accuracy: 0.9625\n",
      "Epoch 137/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.9986 - val_loss: 0.1554 - val_accuracy: 0.9625\n",
      "Epoch 138/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 0.9986 - val_loss: 0.1559 - val_accuracy: 0.9625\n",
      "Epoch 139/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.1638 - val_accuracy: 0.9500\n",
      "Epoch 140/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.1639 - val_accuracy: 0.9625\n",
      "Epoch 141/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.1598 - val_accuracy: 0.9625\n",
      "Epoch 142/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9986 - val_loss: 0.1628 - val_accuracy: 0.9625\n",
      "Epoch 143/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.1674 - val_accuracy: 0.9625\n",
      "Epoch 144/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.1598 - val_accuracy: 0.9750\n",
      "Epoch 145/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.9986 - val_loss: 0.1647 - val_accuracy: 0.9625\n",
      "Epoch 146/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 0.1708 - val_accuracy: 0.9625\n",
      "Epoch 147/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9986 - val_loss: 0.1656 - val_accuracy: 0.9625\n",
      "Epoch 148/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9986 - val_loss: 0.1705 - val_accuracy: 0.9500\n",
      "Epoch 149/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 0.1686 - val_accuracy: 0.9625\n",
      "Epoch 150/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 0.1669 - val_accuracy: 0.9625\n",
      "Epoch 151/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0092 - accuracy: 0.9986 - val_loss: 0.1723 - val_accuracy: 0.9625\n",
      "Epoch 152/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9986 - val_loss: 0.1697 - val_accuracy: 0.9625\n",
      "Epoch 153/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.1697 - val_accuracy: 0.9625\n",
      "Epoch 154/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9986 - val_loss: 0.1694 - val_accuracy: 0.9625\n",
      "Epoch 155/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.1751 - val_accuracy: 0.9625\n",
      "Epoch 156/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.1818 - val_accuracy: 0.9500\n",
      "Epoch 157/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.1785 - val_accuracy: 0.9625\n",
      "Epoch 158/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 0.9986 - val_loss: 0.1879 - val_accuracy: 0.9500\n",
      "Epoch 159/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.1857 - val_accuracy: 0.9500\n",
      "Epoch 160/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9986 - val_loss: 0.1866 - val_accuracy: 0.9500\n",
      "Epoch 161/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.1808 - val_accuracy: 0.9625\n",
      "Epoch 162/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9986 - val_loss: 0.1842 - val_accuracy: 0.9625\n",
      "Epoch 163/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 0.9986 - val_loss: 0.1903 - val_accuracy: 0.9500\n",
      "Epoch 164/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 0.9986 - val_loss: 0.1845 - val_accuracy: 0.9625\n",
      "Epoch 165/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.1978 - val_accuracy: 0.9500\n",
      "Epoch 166/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.1842 - val_accuracy: 0.9625\n",
      "Epoch 167/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 0.9986 - val_loss: 0.1972 - val_accuracy: 0.9500\n",
      "Epoch 168/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.2165 - val_accuracy: 0.9500\n",
      "Epoch 169/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.2013 - val_accuracy: 0.9500\n",
      "Epoch 170/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.2293 - val_accuracy: 0.9625\n",
      "Epoch 171/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.2163 - val_accuracy: 0.9625\n",
      "Epoch 172/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.2310 - val_accuracy: 0.9375\n",
      "Epoch 173/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 0.9986 - val_loss: 0.2228 - val_accuracy: 0.9625\n",
      "Epoch 174/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9500\n",
      "Epoch 175/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.2301 - val_accuracy: 0.9500\n",
      "Epoch 176/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 0.9500\n",
      "Epoch 177/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9500\n",
      "Epoch 178/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9958 - val_loss: 0.2614 - val_accuracy: 0.9500\n",
      "Epoch 179/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.2492 - val_accuracy: 0.9375\n",
      "Epoch 180/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.9625\n",
      "Epoch 181/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.9625\n",
      "Epoch 182/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 0.9625\n",
      "Epoch 183/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9500\n",
      "Epoch 184/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2415 - val_accuracy: 0.9500\n",
      "Epoch 185/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9500\n",
      "Epoch 186/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9500\n",
      "Epoch 187/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9500\n",
      "Epoch 188/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2470 - val_accuracy: 0.9500\n",
      "Epoch 189/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9500\n",
      "Epoch 190/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2534 - val_accuracy: 0.9625\n",
      "Epoch 191/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9500\n",
      "Epoch 192/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2542 - val_accuracy: 0.9500\n",
      "Epoch 193/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.9033e-04 - accuracy: 1.0000 - val_loss: 0.2553 - val_accuracy: 0.9500\n",
      "Epoch 194/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.5995e-04 - accuracy: 1.0000 - val_loss: 0.2563 - val_accuracy: 0.9500\n",
      "Epoch 195/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.5384e-04 - accuracy: 1.0000 - val_loss: 0.2587 - val_accuracy: 0.9500\n",
      "Epoch 196/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 9.3414e-04 - accuracy: 1.0000 - val_loss: 0.2595 - val_accuracy: 0.9500\n",
      "Epoch 197/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 9.3666e-04 - accuracy: 1.0000 - val_loss: 0.2584 - val_accuracy: 0.9500\n",
      "Epoch 198/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 8.6238e-04 - accuracy: 1.0000 - val_loss: 0.2628 - val_accuracy: 0.9500\n",
      "Epoch 199/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 8.3264e-04 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9500\n",
      "Epoch 200/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 7.8877e-04 - accuracy: 1.0000 - val_loss: 0.2649 - val_accuracy: 0.9500\n",
      "Epoch 201/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 9.4046e-04 - accuracy: 1.0000 - val_loss: 0.2643 - val_accuracy: 0.9500\n",
      "Epoch 202/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 8.1776e-04 - accuracy: 1.0000 - val_loss: 0.2681 - val_accuracy: 0.9500\n",
      "Epoch 203/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.7123e-04 - accuracy: 1.0000 - val_loss: 0.2676 - val_accuracy: 0.9500\n",
      "Epoch 204/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 7.1814e-04 - accuracy: 1.0000 - val_loss: 0.2673 - val_accuracy: 0.9500\n",
      "Epoch 205/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 6.6563e-04 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 0.9500\n",
      "Epoch 206/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.4963e-04 - accuracy: 1.0000 - val_loss: 0.2707 - val_accuracy: 0.9500\n",
      "Epoch 207/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.5029e-04 - accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9500\n",
      "Epoch 208/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 6.3356e-04 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 0.9500\n",
      "Epoch 209/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.0781e-04 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9500\n",
      "Epoch 210/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 5.8378e-04 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 0.9500\n",
      "Epoch 211/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.3734e-04 - accuracy: 1.0000 - val_loss: 0.2768 - val_accuracy: 0.9500\n",
      "Epoch 212/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 5.5469e-04 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9500\n",
      "Epoch 213/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 5.4367e-04 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9500\n",
      "Epoch 214/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 5.3262e-04 - accuracy: 1.0000 - val_loss: 0.2786 - val_accuracy: 0.9500\n",
      "Epoch 215/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 5.1869e-04 - accuracy: 1.0000 - val_loss: 0.2811 - val_accuracy: 0.9500\n",
      "Epoch 216/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 5.0895e-04 - accuracy: 1.0000 - val_loss: 0.2794 - val_accuracy: 0.9500\n",
      "Epoch 217/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4.8625e-04 - accuracy: 1.0000 - val_loss: 0.2813 - val_accuracy: 0.9500\n",
      "Epoch 218/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.9576e-04 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9500\n",
      "Epoch 219/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.7410e-04 - accuracy: 1.0000 - val_loss: 0.2825 - val_accuracy: 0.9500\n",
      "Epoch 220/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4.6151e-04 - accuracy: 1.0000 - val_loss: 0.2833 - val_accuracy: 0.9500\n",
      "Epoch 221/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.5720e-04 - accuracy: 1.0000 - val_loss: 0.2858 - val_accuracy: 0.9500\n",
      "Epoch 222/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.3655e-04 - accuracy: 1.0000 - val_loss: 0.2867 - val_accuracy: 0.9500\n",
      "Epoch 223/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.1896e-04 - accuracy: 1.0000 - val_loss: 0.2858 - val_accuracy: 0.9500\n",
      "Epoch 224/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.1215e-04 - accuracy: 1.0000 - val_loss: 0.2874 - val_accuracy: 0.9500\n",
      "Epoch 225/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.0520e-04 - accuracy: 1.0000 - val_loss: 0.2883 - val_accuracy: 0.9500\n",
      "Epoch 226/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2800e-04 - accuracy: 1.0000 - val_loss: 0.2922 - val_accuracy: 0.9375\n",
      "Epoch 227/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.1469e-04 - accuracy: 1.0000 - val_loss: 0.2905 - val_accuracy: 0.9500\n",
      "Epoch 228/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3.8563e-04 - accuracy: 1.0000 - val_loss: 0.2915 - val_accuracy: 0.9500\n",
      "Epoch 229/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.7963e-04 - accuracy: 1.0000 - val_loss: 0.2923 - val_accuracy: 0.9500\n",
      "Epoch 230/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.7264e-04 - accuracy: 1.0000 - val_loss: 0.2923 - val_accuracy: 0.9500\n",
      "Epoch 231/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.7722e-04 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9500\n",
      "Epoch 232/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.6532e-04 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.9500\n",
      "Epoch 233/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.3952e-04 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9500\n",
      "Epoch 234/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3.3472e-04 - accuracy: 1.0000 - val_loss: 0.2968 - val_accuracy: 0.9500\n",
      "Epoch 235/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.2440e-04 - accuracy: 1.0000 - val_loss: 0.2969 - val_accuracy: 0.9500\n",
      "Epoch 236/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.4178e-04 - accuracy: 1.0000 - val_loss: 0.2985 - val_accuracy: 0.9500\n",
      "Epoch 237/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.3098e-04 - accuracy: 1.0000 - val_loss: 0.2993 - val_accuracy: 0.9500\n",
      "Epoch 238/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.2598e-04 - accuracy: 1.0000 - val_loss: 0.2995 - val_accuracy: 0.9500\n",
      "Epoch 239/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.0771e-04 - accuracy: 1.0000 - val_loss: 0.3000 - val_accuracy: 0.9500\n",
      "Epoch 240/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.9875e-04 - accuracy: 1.0000 - val_loss: 0.3016 - val_accuracy: 0.9500\n",
      "Epoch 241/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.9538e-04 - accuracy: 1.0000 - val_loss: 0.3016 - val_accuracy: 0.9500\n",
      "Epoch 242/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.8219e-04 - accuracy: 1.0000 - val_loss: 0.3028 - val_accuracy: 0.9500\n",
      "Epoch 243/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.9050e-04 - accuracy: 1.0000 - val_loss: 0.3060 - val_accuracy: 0.9500\n",
      "Epoch 244/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.8883e-04 - accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9500\n",
      "Epoch 245/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.6906e-04 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9500\n",
      "Epoch 246/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.6334e-04 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9500\n",
      "Epoch 247/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.7632e-04 - accuracy: 1.0000 - val_loss: 0.3087 - val_accuracy: 0.9500\n",
      "Epoch 248/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.6352e-04 - accuracy: 1.0000 - val_loss: 0.3090 - val_accuracy: 0.9500\n",
      "Epoch 249/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.5389e-04 - accuracy: 1.0000 - val_loss: 0.3101 - val_accuracy: 0.9500\n",
      "Epoch 250/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.5206e-04 - accuracy: 1.0000 - val_loss: 0.3098 - val_accuracy: 0.9500\n",
      "Epoch 251/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.3651e-04 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.9500\n",
      "Epoch 252/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.3482e-04 - accuracy: 1.0000 - val_loss: 0.3122 - val_accuracy: 0.9500\n",
      "Epoch 253/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.2500e-04 - accuracy: 1.0000 - val_loss: 0.3119 - val_accuracy: 0.9500\n",
      "Epoch 254/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.3085e-04 - accuracy: 1.0000 - val_loss: 0.3125 - val_accuracy: 0.9500\n",
      "Epoch 255/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.2460e-04 - accuracy: 1.0000 - val_loss: 0.3157 - val_accuracy: 0.9500\n",
      "Epoch 256/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1421e-04 - accuracy: 1.0000 - val_loss: 0.3167 - val_accuracy: 0.9500\n",
      "Epoch 257/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1047e-04 - accuracy: 1.0000 - val_loss: 0.3162 - val_accuracy: 0.9500\n",
      "Epoch 258/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1284e-04 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 0.9500\n",
      "Epoch 259/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1061e-04 - accuracy: 1.0000 - val_loss: 0.3201 - val_accuracy: 0.9500\n",
      "Epoch 260/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.0162e-04 - accuracy: 1.0000 - val_loss: 0.3193 - val_accuracy: 0.9500\n",
      "Epoch 261/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.9464e-04 - accuracy: 1.0000 - val_loss: 0.3201 - val_accuracy: 0.9500\n",
      "Epoch 262/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.8705e-04 - accuracy: 1.0000 - val_loss: 0.3209 - val_accuracy: 0.9500\n",
      "Epoch 263/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.8448e-04 - accuracy: 1.0000 - val_loss: 0.3215 - val_accuracy: 0.9500\n",
      "Epoch 264/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.8044e-04 - accuracy: 1.0000 - val_loss: 0.3227 - val_accuracy: 0.9500\n",
      "Epoch 265/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7645e-04 - accuracy: 1.0000 - val_loss: 0.3239 - val_accuracy: 0.9500\n",
      "Epoch 266/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.8200e-04 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 0.9500\n",
      "Epoch 267/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7524e-04 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 0.9500\n",
      "Epoch 268/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6802e-04 - accuracy: 1.0000 - val_loss: 0.3277 - val_accuracy: 0.9500\n",
      "Epoch 269/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6561e-04 - accuracy: 1.0000 - val_loss: 0.3289 - val_accuracy: 0.9500\n",
      "Epoch 270/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6420e-04 - accuracy: 1.0000 - val_loss: 0.3297 - val_accuracy: 0.9500\n",
      "Epoch 271/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5715e-04 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 0.9500\n",
      "Epoch 272/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5442e-04 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 0.9500\n",
      "Epoch 273/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5257e-04 - accuracy: 1.0000 - val_loss: 0.3312 - val_accuracy: 0.9500\n",
      "Epoch 274/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4535e-04 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 0.9500\n",
      "Epoch 275/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.5064e-04 - accuracy: 1.0000 - val_loss: 0.3336 - val_accuracy: 0.9500\n",
      "Epoch 276/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.4304e-04 - accuracy: 1.0000 - val_loss: 0.3334 - val_accuracy: 0.9500\n",
      "Epoch 277/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3910e-04 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.9500\n",
      "Epoch 278/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3679e-04 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 0.9500\n",
      "Epoch 279/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3447e-04 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9500\n",
      "Epoch 280/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3175e-04 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9500\n",
      "Epoch 281/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2945e-04 - accuracy: 1.0000 - val_loss: 0.3372 - val_accuracy: 0.9500\n",
      "Epoch 282/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.2735e-04 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.9500\n",
      "Epoch 283/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2462e-04 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9500\n",
      "Epoch 284/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2355e-04 - accuracy: 1.0000 - val_loss: 0.3397 - val_accuracy: 0.9500\n",
      "Epoch 285/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.1805e-04 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.9500\n",
      "Epoch 286/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1949e-04 - accuracy: 1.0000 - val_loss: 0.3408 - val_accuracy: 0.9500\n",
      "Epoch 287/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1673e-04 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 0.9500\n",
      "Epoch 288/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1410e-04 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9500\n",
      "Epoch 289/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1567e-04 - accuracy: 1.0000 - val_loss: 0.3418 - val_accuracy: 0.9500\n",
      "Epoch 290/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1087e-04 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9500\n",
      "Epoch 291/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1157e-04 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9500\n",
      "Epoch 292/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.0573e-04 - accuracy: 1.0000 - val_loss: 0.3440 - val_accuracy: 0.9500\n",
      "Epoch 293/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.0372e-04 - accuracy: 1.0000 - val_loss: 0.3454 - val_accuracy: 0.9500\n",
      "Epoch 294/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.0372e-04 - accuracy: 1.0000 - val_loss: 0.3472 - val_accuracy: 0.9500\n",
      "Epoch 295/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0097e-04 - accuracy: 1.0000 - val_loss: 0.3471 - val_accuracy: 0.9500\n",
      "Epoch 296/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 9.8904e-05 - accuracy: 1.0000 - val_loss: 0.3473 - val_accuracy: 0.9500\n",
      "Epoch 297/350\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 9.8641e-05 - accuracy: 1.0000 - val_loss: 0.3479 - val_accuracy: 0.9500\n",
      "Epoch 298/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 9.7156e-05 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.9500\n",
      "Epoch 299/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 9.6845e-05 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.9500\n",
      "Epoch 300/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 9.3726e-05 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 0.9500\n",
      "Epoch 301/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 8.9707e-05 - accuracy: 1.0000 - val_loss: 0.3503 - val_accuracy: 0.9500\n",
      "Epoch 302/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 8.9393e-05 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9500\n",
      "Epoch 303/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.1210e-05 - accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.9500\n",
      "Epoch 304/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 9.4254e-05 - accuracy: 1.0000 - val_loss: 0.3523 - val_accuracy: 0.9500\n",
      "Epoch 305/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 8.5376e-05 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 0.9500\n",
      "Epoch 306/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.2657e-05 - accuracy: 1.0000 - val_loss: 0.3545 - val_accuracy: 0.9500\n",
      "Epoch 307/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.3912e-05 - accuracy: 1.0000 - val_loss: 0.3550 - val_accuracy: 0.9500\n",
      "Epoch 308/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.0768e-05 - accuracy: 1.0000 - val_loss: 0.3562 - val_accuracy: 0.9500\n",
      "Epoch 309/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.1104e-05 - accuracy: 1.0000 - val_loss: 0.3555 - val_accuracy: 0.9500\n",
      "Epoch 310/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.8179e-05 - accuracy: 1.0000 - val_loss: 0.3570 - val_accuracy: 0.9500\n",
      "Epoch 311/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.6135e-05 - accuracy: 1.0000 - val_loss: 0.3579 - val_accuracy: 0.9500\n",
      "Epoch 312/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 7.5784e-05 - accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.9500\n",
      "Epoch 313/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 7.5063e-05 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 0.9500\n",
      "Epoch 314/350\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 7.3761e-05 - accuracy: 1.0000 - val_loss: 0.3602 - val_accuracy: 0.9500\n",
      "Epoch 315/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.1626e-05 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9500\n",
      "Epoch 316/350\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 7.0553e-05 - accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.9500\n",
      "Epoch 317/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.1966e-05 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9500\n",
      "Epoch 318/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.9969e-05 - accuracy: 1.0000 - val_loss: 0.3620 - val_accuracy: 0.9500\n",
      "Epoch 319/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.8893e-05 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 0.9500\n",
      "Epoch 320/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.9057e-05 - accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.9500\n",
      "Epoch 321/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.5537e-05 - accuracy: 1.0000 - val_loss: 0.3631 - val_accuracy: 0.9500\n",
      "Epoch 322/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.5111e-05 - accuracy: 1.0000 - val_loss: 0.3652 - val_accuracy: 0.9500\n",
      "Epoch 323/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.4769e-05 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9500\n",
      "Epoch 324/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.5857e-05 - accuracy: 1.0000 - val_loss: 0.3647 - val_accuracy: 0.9500\n",
      "Epoch 325/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.3572e-05 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 0.9500\n",
      "Epoch 326/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.9888e-05 - accuracy: 1.0000 - val_loss: 0.3673 - val_accuracy: 0.9500\n",
      "Epoch 327/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.7469e-05 - accuracy: 1.0000 - val_loss: 0.3667 - val_accuracy: 0.9500\n",
      "Epoch 328/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.1612e-05 - accuracy: 1.0000 - val_loss: 0.3695 - val_accuracy: 0.9500\n",
      "Epoch 329/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.9008e-05 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 0.9500\n",
      "Epoch 330/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.7360e-05 - accuracy: 1.0000 - val_loss: 0.3713 - val_accuracy: 0.9500\n",
      "Epoch 331/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.7682e-05 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9500\n",
      "Epoch 332/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.4096e-05 - accuracy: 1.0000 - val_loss: 0.3723 - val_accuracy: 0.9500\n",
      "Epoch 333/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.4371e-05 - accuracy: 1.0000 - val_loss: 0.3724 - val_accuracy: 0.9500\n",
      "Epoch 334/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.4460e-05 - accuracy: 1.0000 - val_loss: 0.3734 - val_accuracy: 0.9500\n",
      "Epoch 335/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.3910e-05 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9500\n",
      "Epoch 336/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.2443e-05 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 0.9500\n",
      "Epoch 337/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.3918e-05 - accuracy: 1.0000 - val_loss: 0.3727 - val_accuracy: 0.9500\n",
      "Epoch 338/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.9542e-05 - accuracy: 1.0000 - val_loss: 0.3740 - val_accuracy: 0.9500\n",
      "Epoch 339/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.8862e-05 - accuracy: 1.0000 - val_loss: 0.3748 - val_accuracy: 0.9500\n",
      "Epoch 340/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.8823e-05 - accuracy: 1.0000 - val_loss: 0.3772 - val_accuracy: 0.9500\n",
      "Epoch 341/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.8604e-05 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9500\n",
      "Epoch 342/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.4605e-05 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 0.9500\n",
      "Epoch 343/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.7241e-05 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 0.9500\n",
      "Epoch 344/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4.5796e-05 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9500\n",
      "Epoch 345/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.5706e-05 - accuracy: 1.0000 - val_loss: 0.3804 - val_accuracy: 0.9500\n",
      "Epoch 346/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.3157e-05 - accuracy: 1.0000 - val_loss: 0.3800 - val_accuracy: 0.9500\n",
      "Epoch 347/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2797e-05 - accuracy: 1.0000 - val_loss: 0.3801 - val_accuracy: 0.9500\n",
      "Epoch 348/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2484e-05 - accuracy: 1.0000 - val_loss: 0.3811 - val_accuracy: 0.9500\n",
      "Epoch 349/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2158e-05 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9500\n",
      "Epoch 350/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4.1674e-05 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train, validation_split=0.1,epochs=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "635a6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24deaca0990>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the train accuracy and test accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f585fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step - loss: 1.1660 - accuracy: 0.9196\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9e82275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9195979833602905\n"
     ]
    }
   ],
   "source": [
    "# Accuracy was 91.9%\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974cc7e",
   "metadata": {},
   "source": [
    "## Conclusion :\n",
    "## Neural Net produced 91.9% accuracy , whereas Random Forest produced 93% accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
