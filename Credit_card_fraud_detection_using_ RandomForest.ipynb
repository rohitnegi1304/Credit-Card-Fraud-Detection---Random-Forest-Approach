{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf031628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d459b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:/Users/91745/Downloads/archive_creditcard/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03880c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.449044</td>\n",
       "      <td>-1.176339</td>\n",
       "      <td>0.913860</td>\n",
       "      <td>-1.375667</td>\n",
       "      <td>-1.971383</td>\n",
       "      <td>-0.629152</td>\n",
       "      <td>-1.423236</td>\n",
       "      <td>0.048456</td>\n",
       "      <td>-1.720408</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009302</td>\n",
       "      <td>0.313894</td>\n",
       "      <td>0.027740</td>\n",
       "      <td>0.500512</td>\n",
       "      <td>0.251367</td>\n",
       "      <td>-0.129478</td>\n",
       "      <td>0.042850</td>\n",
       "      <td>0.016253</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.384978</td>\n",
       "      <td>0.616109</td>\n",
       "      <td>-0.874300</td>\n",
       "      <td>-0.094019</td>\n",
       "      <td>2.924584</td>\n",
       "      <td>3.317027</td>\n",
       "      <td>0.470455</td>\n",
       "      <td>0.538247</td>\n",
       "      <td>-0.558895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049924</td>\n",
       "      <td>0.238422</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>0.996710</td>\n",
       "      <td>-0.767315</td>\n",
       "      <td>-0.492208</td>\n",
       "      <td>0.042472</td>\n",
       "      <td>-0.054337</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.249999</td>\n",
       "      <td>-1.221637</td>\n",
       "      <td>0.383930</td>\n",
       "      <td>-1.234899</td>\n",
       "      <td>-1.485419</td>\n",
       "      <td>-0.753230</td>\n",
       "      <td>-0.689405</td>\n",
       "      <td>-0.227487</td>\n",
       "      <td>-2.094011</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231809</td>\n",
       "      <td>-0.483285</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.161135</td>\n",
       "      <td>-0.354990</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>121.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1.069374</td>\n",
       "      <td>0.287722</td>\n",
       "      <td>0.828613</td>\n",
       "      <td>2.712520</td>\n",
       "      <td>-0.178398</td>\n",
       "      <td>0.337544</td>\n",
       "      <td>-0.096717</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>-0.221083</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036876</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.071407</td>\n",
       "      <td>0.104744</td>\n",
       "      <td>0.548265</td>\n",
       "      <td>0.104094</td>\n",
       "      <td>0.021491</td>\n",
       "      <td>0.021293</td>\n",
       "      <td>27.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-2.791855</td>\n",
       "      <td>-0.327771</td>\n",
       "      <td>1.641750</td>\n",
       "      <td>1.767473</td>\n",
       "      <td>-0.136588</td>\n",
       "      <td>0.807596</td>\n",
       "      <td>-0.422911</td>\n",
       "      <td>-1.907107</td>\n",
       "      <td>0.755713</td>\n",
       "      <td>...</td>\n",
       "      <td>1.151663</td>\n",
       "      <td>0.222182</td>\n",
       "      <td>1.020586</td>\n",
       "      <td>0.028317</td>\n",
       "      <td>-0.232746</td>\n",
       "      <td>-0.235557</td>\n",
       "      <td>-0.164778</td>\n",
       "      <td>-0.030154</td>\n",
       "      <td>58.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.752417</td>\n",
       "      <td>0.345485</td>\n",
       "      <td>2.057323</td>\n",
       "      <td>-1.468643</td>\n",
       "      <td>-1.158394</td>\n",
       "      <td>-0.077850</td>\n",
       "      <td>-0.608581</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>-0.436167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>1.353650</td>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.065084</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>-0.087086</td>\n",
       "      <td>-0.180998</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>15.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1.103215</td>\n",
       "      <td>-0.040296</td>\n",
       "      <td>1.267332</td>\n",
       "      <td>1.289091</td>\n",
       "      <td>-0.735997</td>\n",
       "      <td>0.288069</td>\n",
       "      <td>-0.586057</td>\n",
       "      <td>0.189380</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024612</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.103758</td>\n",
       "      <td>0.364298</td>\n",
       "      <td>-0.382261</td>\n",
       "      <td>0.092809</td>\n",
       "      <td>0.037051</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.436905</td>\n",
       "      <td>0.918966</td>\n",
       "      <td>0.924591</td>\n",
       "      <td>-0.727219</td>\n",
       "      <td>0.915679</td>\n",
       "      <td>-0.127867</td>\n",
       "      <td>0.707642</td>\n",
       "      <td>0.087962</td>\n",
       "      <td>-0.665271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194796</td>\n",
       "      <td>-0.672638</td>\n",
       "      <td>-0.156858</td>\n",
       "      <td>-0.888386</td>\n",
       "      <td>-0.342413</td>\n",
       "      <td>-0.049027</td>\n",
       "      <td>0.079692</td>\n",
       "      <td>0.131024</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.0</td>\n",
       "      <td>-5.401258</td>\n",
       "      <td>-5.450148</td>\n",
       "      <td>1.186305</td>\n",
       "      <td>1.736239</td>\n",
       "      <td>3.049106</td>\n",
       "      <td>-1.763406</td>\n",
       "      <td>-1.559738</td>\n",
       "      <td>0.160842</td>\n",
       "      <td>1.233090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503600</td>\n",
       "      <td>0.984460</td>\n",
       "      <td>2.458589</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>-0.481631</td>\n",
       "      <td>-0.621272</td>\n",
       "      <td>0.392053</td>\n",
       "      <td>0.949594</td>\n",
       "      <td>46.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.492936</td>\n",
       "      <td>-1.029346</td>\n",
       "      <td>0.454795</td>\n",
       "      <td>-1.438026</td>\n",
       "      <td>-1.555434</td>\n",
       "      <td>-0.720961</td>\n",
       "      <td>-1.080664</td>\n",
       "      <td>-0.053127</td>\n",
       "      <td>-1.978682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177650</td>\n",
       "      <td>-0.175074</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.295814</td>\n",
       "      <td>0.332931</td>\n",
       "      <td>-0.220385</td>\n",
       "      <td>0.022298</td>\n",
       "      <td>0.007602</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.694885</td>\n",
       "      <td>-1.361819</td>\n",
       "      <td>1.029221</td>\n",
       "      <td>0.834159</td>\n",
       "      <td>-1.191209</td>\n",
       "      <td>1.309109</td>\n",
       "      <td>-0.878586</td>\n",
       "      <td>0.445290</td>\n",
       "      <td>-0.446196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295583</td>\n",
       "      <td>-0.571955</td>\n",
       "      <td>-0.050881</td>\n",
       "      <td>-0.304215</td>\n",
       "      <td>0.072001</td>\n",
       "      <td>-0.422234</td>\n",
       "      <td>0.086553</td>\n",
       "      <td>0.063499</td>\n",
       "      <td>231.71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>17.0</td>\n",
       "      <td>0.962496</td>\n",
       "      <td>0.328461</td>\n",
       "      <td>-0.171479</td>\n",
       "      <td>2.109204</td>\n",
       "      <td>1.129566</td>\n",
       "      <td>1.696038</td>\n",
       "      <td>0.107712</td>\n",
       "      <td>0.521502</td>\n",
       "      <td>-1.191311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143997</td>\n",
       "      <td>0.402492</td>\n",
       "      <td>-0.048508</td>\n",
       "      <td>-1.371866</td>\n",
       "      <td>0.390814</td>\n",
       "      <td>0.199964</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>-0.014605</td>\n",
       "      <td>34.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.166616</td>\n",
       "      <td>0.502120</td>\n",
       "      <td>-0.067300</td>\n",
       "      <td>2.261569</td>\n",
       "      <td>0.428804</td>\n",
       "      <td>0.089474</td>\n",
       "      <td>0.241147</td>\n",
       "      <td>0.138082</td>\n",
       "      <td>-0.989162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>-0.061972</td>\n",
       "      <td>-0.103855</td>\n",
       "      <td>-0.370415</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.108556</td>\n",
       "      <td>-0.040521</td>\n",
       "      <td>-0.011418</td>\n",
       "      <td>2.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.247491</td>\n",
       "      <td>0.277666</td>\n",
       "      <td>1.185471</td>\n",
       "      <td>-0.092603</td>\n",
       "      <td>-1.314394</td>\n",
       "      <td>-0.150116</td>\n",
       "      <td>-0.946365</td>\n",
       "      <td>-1.617935</td>\n",
       "      <td>1.544071</td>\n",
       "      <td>...</td>\n",
       "      <td>1.650180</td>\n",
       "      <td>0.200454</td>\n",
       "      <td>-0.185353</td>\n",
       "      <td>0.423073</td>\n",
       "      <td>0.820591</td>\n",
       "      <td>-0.227632</td>\n",
       "      <td>0.336634</td>\n",
       "      <td>0.250475</td>\n",
       "      <td>22.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22.0</td>\n",
       "      <td>-1.946525</td>\n",
       "      <td>-0.044901</td>\n",
       "      <td>-0.405570</td>\n",
       "      <td>-1.013057</td>\n",
       "      <td>2.941968</td>\n",
       "      <td>2.955053</td>\n",
       "      <td>-0.063063</td>\n",
       "      <td>0.855546</td>\n",
       "      <td>0.049967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579526</td>\n",
       "      <td>-0.799229</td>\n",
       "      <td>0.870300</td>\n",
       "      <td>0.983421</td>\n",
       "      <td>0.321201</td>\n",
       "      <td>0.149650</td>\n",
       "      <td>0.707519</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0    0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1    0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2    1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3    1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4    2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "5    2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728   \n",
       "6    4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708   \n",
       "7    7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118   \n",
       "8    7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818   \n",
       "9    9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761   \n",
       "10  10.0  1.449044 -1.176339  0.913860 -1.375667 -1.971383 -0.629152   \n",
       "11  10.0  0.384978  0.616109 -0.874300 -0.094019  2.924584  3.317027   \n",
       "12  10.0  1.249999 -1.221637  0.383930 -1.234899 -1.485419 -0.753230   \n",
       "13  11.0  1.069374  0.287722  0.828613  2.712520 -0.178398  0.337544   \n",
       "14  12.0 -2.791855 -0.327771  1.641750  1.767473 -0.136588  0.807596   \n",
       "15  12.0 -0.752417  0.345485  2.057323 -1.468643 -1.158394 -0.077850   \n",
       "16  12.0  1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069   \n",
       "17  13.0 -0.436905  0.918966  0.924591 -0.727219  0.915679 -0.127867   \n",
       "18  14.0 -5.401258 -5.450148  1.186305  1.736239  3.049106 -1.763406   \n",
       "19  15.0  1.492936 -1.029346  0.454795 -1.438026 -1.555434 -0.720961   \n",
       "20  16.0  0.694885 -1.361819  1.029221  0.834159 -1.191209  1.309109   \n",
       "21  17.0  0.962496  0.328461 -0.171479  2.109204  1.129566  1.696038   \n",
       "22  18.0  1.166616  0.502120 -0.067300  2.261569  0.428804  0.089474   \n",
       "23  18.0  0.247491  0.277666  1.185471 -0.092603 -1.314394 -0.150116   \n",
       "24  22.0 -1.946525 -0.044901 -0.405570 -1.013057  2.941968  2.955053   \n",
       "\n",
       "          V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0   0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2   0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3   0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4   0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "5   0.476201  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427   \n",
       "6  -0.005159  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055   \n",
       "7   1.120631 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709   \n",
       "8   0.370145  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592   \n",
       "9   0.651583  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050   \n",
       "10 -1.423236  0.048456 -1.720408  ... -0.009302  0.313894  0.027740  0.500512   \n",
       "11  0.470455  0.538247 -0.558895  ...  0.049924  0.238422  0.009130  0.996710   \n",
       "12 -0.689405 -0.227487 -2.094011  ... -0.231809 -0.483285  0.084668  0.392831   \n",
       "13 -0.096717  0.115982 -0.221083  ... -0.036876  0.074412 -0.071407  0.104744   \n",
       "14 -0.422911 -1.907107  0.755713  ...  1.151663  0.222182  1.020586  0.028317   \n",
       "15 -0.608581  0.003603 -0.436167  ...  0.499625  1.353650 -0.256573 -0.065084   \n",
       "16 -0.586057  0.189380  0.782333  ... -0.024612  0.196002  0.013802  0.103758   \n",
       "17  0.707642  0.087962 -0.665271  ... -0.194796 -0.672638 -0.156858 -0.888386   \n",
       "18 -1.559738  0.160842  1.233090  ... -0.503600  0.984460  2.458589  0.042119   \n",
       "19 -1.080664 -0.053127 -1.978682  ... -0.177650 -0.175074  0.040002  0.295814   \n",
       "20 -0.878586  0.445290 -0.446196  ... -0.295583 -0.571955 -0.050881 -0.304215   \n",
       "21  0.107712  0.521502 -1.191311  ...  0.143997  0.402492 -0.048508 -1.371866   \n",
       "22  0.241147  0.138082 -0.989162  ...  0.018702 -0.061972 -0.103855 -0.370415   \n",
       "23 -0.946365 -1.617935  1.544071  ...  1.650180  0.200454 -0.185353  0.423073   \n",
       "24 -0.063063  0.855546  0.049967  ... -0.579526 -0.799229  0.870300  0.983421   \n",
       "\n",
       "         V25       V26       V27       V28  Amount  Class  \n",
       "0   0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1   0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2  -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3   0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "5  -0.232794  0.105915  0.253844  0.081080    3.67      0  \n",
       "6   0.750137 -0.257237  0.034507  0.005168    4.99      0  \n",
       "7  -0.415267 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "8   0.373205 -0.384157  0.011747  0.142404   93.20      0  \n",
       "9  -0.069733  0.094199  0.246219  0.083076    3.68      0  \n",
       "10  0.251367 -0.129478  0.042850  0.016253    7.80      0  \n",
       "11 -0.767315 -0.492208  0.042472 -0.054337    9.99      0  \n",
       "12  0.161135 -0.354990  0.026416  0.042422  121.50      0  \n",
       "13  0.548265  0.104094  0.021491  0.021293   27.50      0  \n",
       "14 -0.232746 -0.235557 -0.164778 -0.030154   58.80      0  \n",
       "15 -0.039124 -0.087086 -0.180998  0.129394   15.99      0  \n",
       "16  0.364298 -0.382261  0.092809  0.037051   12.99      0  \n",
       "17 -0.342413 -0.049027  0.079692  0.131024    0.89      0  \n",
       "18 -0.481631 -0.621272  0.392053  0.949594   46.80      0  \n",
       "19  0.332931 -0.220385  0.022298  0.007602    5.00      0  \n",
       "20  0.072001 -0.422234  0.086553  0.063499  231.71      0  \n",
       "21  0.390814  0.199964  0.016371 -0.014605   34.09      0  \n",
       "22  0.603200  0.108556 -0.040521 -0.011418    2.28      0  \n",
       "23  0.820591 -0.227632  0.336634  0.250475   22.75      0  \n",
       "24  0.321201  0.149650  0.707519  0.014600    0.89      0  \n",
       "\n",
       "[25 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9c06e31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284782</th>\n",
       "      <td>172767.0</td>\n",
       "      <td>-0.268061</td>\n",
       "      <td>2.540315</td>\n",
       "      <td>-1.400915</td>\n",
       "      <td>4.846661</td>\n",
       "      <td>0.639105</td>\n",
       "      <td>0.186479</td>\n",
       "      <td>-0.045911</td>\n",
       "      <td>0.936448</td>\n",
       "      <td>-2.419986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.263889</td>\n",
       "      <td>-0.857904</td>\n",
       "      <td>0.235172</td>\n",
       "      <td>-0.681794</td>\n",
       "      <td>-0.668894</td>\n",
       "      <td>0.044657</td>\n",
       "      <td>-0.066751</td>\n",
       "      <td>-0.072447</td>\n",
       "      <td>12.82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284783</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-1.796092</td>\n",
       "      <td>1.929178</td>\n",
       "      <td>-2.828417</td>\n",
       "      <td>-1.689844</td>\n",
       "      <td>2.199572</td>\n",
       "      <td>3.123732</td>\n",
       "      <td>-0.270714</td>\n",
       "      <td>1.657495</td>\n",
       "      <td>0.465804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271170</td>\n",
       "      <td>1.145750</td>\n",
       "      <td>0.084783</td>\n",
       "      <td>0.721269</td>\n",
       "      <td>-0.529906</td>\n",
       "      <td>-0.240117</td>\n",
       "      <td>0.129126</td>\n",
       "      <td>-0.080620</td>\n",
       "      <td>11.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284784</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-0.669662</td>\n",
       "      <td>0.923769</td>\n",
       "      <td>-1.543167</td>\n",
       "      <td>-1.560729</td>\n",
       "      <td>2.833960</td>\n",
       "      <td>3.240843</td>\n",
       "      <td>0.181576</td>\n",
       "      <td>1.282746</td>\n",
       "      <td>-0.893890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183856</td>\n",
       "      <td>0.202670</td>\n",
       "      <td>-0.373023</td>\n",
       "      <td>0.651122</td>\n",
       "      <td>1.073823</td>\n",
       "      <td>0.844590</td>\n",
       "      <td>-0.286676</td>\n",
       "      <td>-0.187719</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284785</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>0.032887</td>\n",
       "      <td>0.545338</td>\n",
       "      <td>-1.185844</td>\n",
       "      <td>-1.729828</td>\n",
       "      <td>2.932315</td>\n",
       "      <td>3.401529</td>\n",
       "      <td>0.337434</td>\n",
       "      <td>0.925377</td>\n",
       "      <td>-0.165663</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266113</td>\n",
       "      <td>-0.716336</td>\n",
       "      <td>0.108519</td>\n",
       "      <td>0.688519</td>\n",
       "      <td>-0.460220</td>\n",
       "      <td>0.161939</td>\n",
       "      <td>0.265368</td>\n",
       "      <td>0.090245</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284786</th>\n",
       "      <td>172768.0</td>\n",
       "      <td>-2.076175</td>\n",
       "      <td>2.142238</td>\n",
       "      <td>-2.522704</td>\n",
       "      <td>-1.888063</td>\n",
       "      <td>1.982785</td>\n",
       "      <td>3.732950</td>\n",
       "      <td>-1.217430</td>\n",
       "      <td>-0.536644</td>\n",
       "      <td>0.272867</td>\n",
       "      <td>...</td>\n",
       "      <td>2.016666</td>\n",
       "      <td>-1.588269</td>\n",
       "      <td>0.588482</td>\n",
       "      <td>0.632444</td>\n",
       "      <td>-0.201064</td>\n",
       "      <td>0.199251</td>\n",
       "      <td>0.438657</td>\n",
       "      <td>0.172923</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284787</th>\n",
       "      <td>172769.0</td>\n",
       "      <td>-1.029719</td>\n",
       "      <td>-1.110670</td>\n",
       "      <td>-0.636179</td>\n",
       "      <td>-0.840816</td>\n",
       "      <td>2.424360</td>\n",
       "      <td>-2.956733</td>\n",
       "      <td>0.283610</td>\n",
       "      <td>-0.332656</td>\n",
       "      <td>-0.247488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.353722</td>\n",
       "      <td>0.488487</td>\n",
       "      <td>0.293632</td>\n",
       "      <td>0.107812</td>\n",
       "      <td>-0.935586</td>\n",
       "      <td>1.138216</td>\n",
       "      <td>0.025271</td>\n",
       "      <td>0.255347</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284788</th>\n",
       "      <td>172770.0</td>\n",
       "      <td>2.007418</td>\n",
       "      <td>-0.280235</td>\n",
       "      <td>-0.208113</td>\n",
       "      <td>0.335261</td>\n",
       "      <td>-0.715798</td>\n",
       "      <td>-0.751373</td>\n",
       "      <td>-0.458972</td>\n",
       "      <td>-0.140140</td>\n",
       "      <td>0.959971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208260</td>\n",
       "      <td>-0.430347</td>\n",
       "      <td>0.416765</td>\n",
       "      <td>0.064819</td>\n",
       "      <td>-0.608337</td>\n",
       "      <td>0.268436</td>\n",
       "      <td>-0.028069</td>\n",
       "      <td>-0.041367</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284789</th>\n",
       "      <td>172770.0</td>\n",
       "      <td>-0.446951</td>\n",
       "      <td>1.302212</td>\n",
       "      <td>-0.168583</td>\n",
       "      <td>0.981577</td>\n",
       "      <td>0.578957</td>\n",
       "      <td>-0.605641</td>\n",
       "      <td>1.253430</td>\n",
       "      <td>-1.042610</td>\n",
       "      <td>-0.417116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.851800</td>\n",
       "      <td>0.305268</td>\n",
       "      <td>-0.148093</td>\n",
       "      <td>-0.038712</td>\n",
       "      <td>0.010209</td>\n",
       "      <td>-0.362666</td>\n",
       "      <td>0.503092</td>\n",
       "      <td>0.229921</td>\n",
       "      <td>60.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284790</th>\n",
       "      <td>172771.0</td>\n",
       "      <td>-0.515513</td>\n",
       "      <td>0.971950</td>\n",
       "      <td>-1.014580</td>\n",
       "      <td>-0.677037</td>\n",
       "      <td>0.912430</td>\n",
       "      <td>-0.316187</td>\n",
       "      <td>0.396137</td>\n",
       "      <td>0.532364</td>\n",
       "      <td>-0.224606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280302</td>\n",
       "      <td>-0.849919</td>\n",
       "      <td>0.300245</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>-0.376379</td>\n",
       "      <td>0.128660</td>\n",
       "      <td>-0.015205</td>\n",
       "      <td>-0.021486</td>\n",
       "      <td>9.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284791</th>\n",
       "      <td>172774.0</td>\n",
       "      <td>-0.863506</td>\n",
       "      <td>0.874701</td>\n",
       "      <td>0.420358</td>\n",
       "      <td>-0.530365</td>\n",
       "      <td>0.356561</td>\n",
       "      <td>-1.046238</td>\n",
       "      <td>0.757051</td>\n",
       "      <td>0.230473</td>\n",
       "      <td>-0.506856</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108846</td>\n",
       "      <td>-0.480820</td>\n",
       "      <td>-0.074513</td>\n",
       "      <td>-0.003988</td>\n",
       "      <td>-0.113149</td>\n",
       "      <td>0.280378</td>\n",
       "      <td>-0.077310</td>\n",
       "      <td>0.023079</td>\n",
       "      <td>20.32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284792</th>\n",
       "      <td>172774.0</td>\n",
       "      <td>-0.724123</td>\n",
       "      <td>1.485216</td>\n",
       "      <td>-1.132218</td>\n",
       "      <td>-0.607190</td>\n",
       "      <td>0.709499</td>\n",
       "      <td>-0.482638</td>\n",
       "      <td>0.548393</td>\n",
       "      <td>0.343003</td>\n",
       "      <td>-0.226323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414621</td>\n",
       "      <td>1.307511</td>\n",
       "      <td>-0.059545</td>\n",
       "      <td>0.242669</td>\n",
       "      <td>-0.665424</td>\n",
       "      <td>-0.269869</td>\n",
       "      <td>-0.170579</td>\n",
       "      <td>-0.030692</td>\n",
       "      <td>3.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284793</th>\n",
       "      <td>172775.0</td>\n",
       "      <td>1.971002</td>\n",
       "      <td>-0.699067</td>\n",
       "      <td>-1.697541</td>\n",
       "      <td>-0.617643</td>\n",
       "      <td>1.718797</td>\n",
       "      <td>3.911336</td>\n",
       "      <td>-1.259306</td>\n",
       "      <td>1.056209</td>\n",
       "      <td>1.315006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188758</td>\n",
       "      <td>0.694418</td>\n",
       "      <td>0.163002</td>\n",
       "      <td>0.726365</td>\n",
       "      <td>-0.058282</td>\n",
       "      <td>-0.191813</td>\n",
       "      <td>0.061858</td>\n",
       "      <td>-0.043716</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284794</th>\n",
       "      <td>172777.0</td>\n",
       "      <td>-1.266580</td>\n",
       "      <td>-0.400461</td>\n",
       "      <td>0.956221</td>\n",
       "      <td>-0.723919</td>\n",
       "      <td>1.531993</td>\n",
       "      <td>-1.788600</td>\n",
       "      <td>0.314741</td>\n",
       "      <td>0.004704</td>\n",
       "      <td>0.013857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157831</td>\n",
       "      <td>-0.883365</td>\n",
       "      <td>0.088485</td>\n",
       "      <td>-0.076790</td>\n",
       "      <td>-0.095833</td>\n",
       "      <td>0.132720</td>\n",
       "      <td>-0.028468</td>\n",
       "      <td>0.126494</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284795</th>\n",
       "      <td>172778.0</td>\n",
       "      <td>-12.516732</td>\n",
       "      <td>10.187818</td>\n",
       "      <td>-8.476671</td>\n",
       "      <td>-2.510473</td>\n",
       "      <td>-4.586669</td>\n",
       "      <td>-1.394465</td>\n",
       "      <td>-3.632516</td>\n",
       "      <td>5.498583</td>\n",
       "      <td>4.893089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.944759</td>\n",
       "      <td>-1.565026</td>\n",
       "      <td>0.890675</td>\n",
       "      <td>-1.253276</td>\n",
       "      <td>1.786717</td>\n",
       "      <td>0.320763</td>\n",
       "      <td>2.090712</td>\n",
       "      <td>1.232864</td>\n",
       "      <td>9.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284796</th>\n",
       "      <td>172780.0</td>\n",
       "      <td>1.884849</td>\n",
       "      <td>-0.143540</td>\n",
       "      <td>-0.999943</td>\n",
       "      <td>1.506772</td>\n",
       "      <td>-0.035300</td>\n",
       "      <td>-0.613638</td>\n",
       "      <td>0.190241</td>\n",
       "      <td>-0.249058</td>\n",
       "      <td>0.666458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144008</td>\n",
       "      <td>0.634646</td>\n",
       "      <td>-0.042114</td>\n",
       "      <td>-0.053206</td>\n",
       "      <td>0.316403</td>\n",
       "      <td>-0.461441</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>-0.041068</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284797</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>-0.241923</td>\n",
       "      <td>0.712247</td>\n",
       "      <td>0.399806</td>\n",
       "      <td>-0.463406</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>-1.343668</td>\n",
       "      <td>0.929369</td>\n",
       "      <td>-0.206210</td>\n",
       "      <td>0.106234</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228876</td>\n",
       "      <td>-0.514376</td>\n",
       "      <td>0.279598</td>\n",
       "      <td>0.371441</td>\n",
       "      <td>-0.559238</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>0.131507</td>\n",
       "      <td>0.081265</td>\n",
       "      <td>5.49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284798</th>\n",
       "      <td>172782.0</td>\n",
       "      <td>0.219529</td>\n",
       "      <td>0.881246</td>\n",
       "      <td>-0.635891</td>\n",
       "      <td>0.960928</td>\n",
       "      <td>-0.152971</td>\n",
       "      <td>-1.014307</td>\n",
       "      <td>0.427126</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>-0.285670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099936</td>\n",
       "      <td>0.337120</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>0.057688</td>\n",
       "      <td>-1.508368</td>\n",
       "      <td>0.144023</td>\n",
       "      <td>0.181205</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>24.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284799</th>\n",
       "      <td>172783.0</td>\n",
       "      <td>-1.775135</td>\n",
       "      <td>-0.004235</td>\n",
       "      <td>1.189786</td>\n",
       "      <td>0.331096</td>\n",
       "      <td>1.196063</td>\n",
       "      <td>5.519980</td>\n",
       "      <td>-1.518185</td>\n",
       "      <td>2.080825</td>\n",
       "      <td>1.159498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103302</td>\n",
       "      <td>0.654850</td>\n",
       "      <td>-0.348929</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.704545</td>\n",
       "      <td>-0.127579</td>\n",
       "      <td>0.454379</td>\n",
       "      <td>0.130308</td>\n",
       "      <td>79.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284800</th>\n",
       "      <td>172784.0</td>\n",
       "      <td>2.039560</td>\n",
       "      <td>-0.175233</td>\n",
       "      <td>-1.196825</td>\n",
       "      <td>0.234580</td>\n",
       "      <td>-0.008713</td>\n",
       "      <td>-0.726571</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>-0.118228</td>\n",
       "      <td>0.435402</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268048</td>\n",
       "      <td>-0.717211</td>\n",
       "      <td>0.297930</td>\n",
       "      <td>-0.359769</td>\n",
       "      <td>-0.315610</td>\n",
       "      <td>0.201114</td>\n",
       "      <td>-0.080826</td>\n",
       "      <td>-0.075071</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284801</th>\n",
       "      <td>172785.0</td>\n",
       "      <td>0.120316</td>\n",
       "      <td>0.931005</td>\n",
       "      <td>-0.546012</td>\n",
       "      <td>-0.745097</td>\n",
       "      <td>1.130314</td>\n",
       "      <td>-0.235973</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>0.115093</td>\n",
       "      <td>-0.204064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314205</td>\n",
       "      <td>-0.808520</td>\n",
       "      <td>0.050343</td>\n",
       "      <td>0.102800</td>\n",
       "      <td>-0.435870</td>\n",
       "      <td>0.124079</td>\n",
       "      <td>0.217940</td>\n",
       "      <td>0.068803</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n",
       "284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n",
       "284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n",
       "284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n",
       "284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n",
       "284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n",
       "284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n",
       "284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n",
       "284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n",
       "284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n",
       "284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n",
       "284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n",
       "284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n",
       "284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n",
       "284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n",
       "284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n",
       "284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n",
       "284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n",
       "284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n",
       "284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "284782  0.186479 -0.045911  0.936448 -2.419986  ... -0.263889 -0.857904   \n",
       "284783  3.123732 -0.270714  1.657495  0.465804  ...  0.271170  1.145750   \n",
       "284784  3.240843  0.181576  1.282746 -0.893890  ...  0.183856  0.202670   \n",
       "284785  3.401529  0.337434  0.925377 -0.165663  ... -0.266113 -0.716336   \n",
       "284786  3.732950 -1.217430 -0.536644  0.272867  ...  2.016666 -1.588269   \n",
       "284787 -2.956733  0.283610 -0.332656 -0.247488  ...  0.353722  0.488487   \n",
       "284788 -0.751373 -0.458972 -0.140140  0.959971  ... -0.208260 -0.430347   \n",
       "284789 -0.605641  1.253430 -1.042610 -0.417116  ...  0.851800  0.305268   \n",
       "284790 -0.316187  0.396137  0.532364 -0.224606  ... -0.280302 -0.849919   \n",
       "284791 -1.046238  0.757051  0.230473 -0.506856  ... -0.108846 -0.480820   \n",
       "284792 -0.482638  0.548393  0.343003 -0.226323  ...  0.414621  1.307511   \n",
       "284793  3.911336 -1.259306  1.056209  1.315006  ...  0.188758  0.694418   \n",
       "284794 -1.788600  0.314741  0.004704  0.013857  ... -0.157831 -0.883365   \n",
       "284795 -1.394465 -3.632516  5.498583  4.893089  ... -0.944759 -1.565026   \n",
       "284796 -0.613638  0.190241 -0.249058  0.666458  ...  0.144008  0.634646   \n",
       "284797 -1.343668  0.929369 -0.206210  0.106234  ... -0.228876 -0.514376   \n",
       "284798 -1.014307  0.427126  0.121340 -0.285670  ...  0.099936  0.337120   \n",
       "284799  5.519980 -1.518185  2.080825  1.159498  ...  0.103302  0.654850   \n",
       "284800 -0.726571  0.017050 -0.118228  0.435402  ... -0.268048 -0.717211   \n",
       "284801 -0.235973  0.812722  0.115093 -0.204064  ... -0.314205 -0.808520   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "284782  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n",
       "284783  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n",
       "284784 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n",
       "284785  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245    1.79   \n",
       "284786  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n",
       "284787  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n",
       "284788  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n",
       "284789 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n",
       "284790  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n",
       "284791 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n",
       "284792 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n",
       "284793  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n",
       "284794  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n",
       "284795  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n",
       "284796 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n",
       "284797  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n",
       "284798  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n",
       "284799 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   79.99   \n",
       "284800  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071    2.68   \n",
       "284801  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803    2.69   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "284782      0  \n",
       "284783      0  \n",
       "284784      0  \n",
       "284785      0  \n",
       "284786      0  \n",
       "284787      0  \n",
       "284788      0  \n",
       "284789      0  \n",
       "284790      0  \n",
       "284791      0  \n",
       "284792      0  \n",
       "284793      0  \n",
       "284794      0  \n",
       "284795      0  \n",
       "284796      0  \n",
       "284797      0  \n",
       "284798      0  \n",
       "284799      0  \n",
       "284800      0  \n",
       "284801      0  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[25 rows x 31 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75eba1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fca07e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "613c4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fraud: (284315, 31)\n",
      "Shape of nonfraud: (492, 31)\n"
     ]
    }
   ],
   "source": [
    "#Dummy\n",
    "fraud_credit_data = dataset[dataset['Class']==0]\n",
    "nonfraud_credit_data = dataset[dataset['Class']==1]\n",
    "\n",
    "print(\"Shape of fraud:\",fraud_credit_data.shape)\n",
    "print(\"Shape of nonfraud:\",nonfraud_credit_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d38c67cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4920</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6108</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6329</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Time        V1        V2        V3        V4        V5        V6  \\\n",
       "541    406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "623    472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4920  4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6108  6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6329  7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "\n",
       "            V7        V8        V9  ...       V21       V22       V23  \\\n",
       "541  -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "623   0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "4920  0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726   \n",
       "6108 -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207   \n",
       "6329  1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805   \n",
       "\n",
       "           V24       V25       V26       V27       V28  Amount  Class  \n",
       "541   0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "623  -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4920 -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6108 -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6329 -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonfraud_credit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e543f14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_sample = fraud_credit_data.sample(n=500)\n",
    "fraud_sample.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a98a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(992, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset=pd.concat([fraud_sample,nonfraud_credit_data],axis=0)\n",
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d5fba1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=new_dataset.drop(columns=['Class'],axis=1)\n",
    "Y=new_dataset['Class']\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=3)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27f72ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "643e6e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dcef4829",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[LogisticRegression(max_iter=1000000),LinearSVC(),RandomForestClassifier(),MLPClassifier(),GaussianNB(),DecisionTreeClassifier()]\n",
    "\n",
    "def classifier_model():\n",
    "\n",
    "  for model in model_list:\n",
    "    model.fit(X_train,Y_train)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    acc_score_train = accuracy_score(Y_train,y_train_pred)\n",
    "    acc_score_test = accuracy_score(Y_test,y_test_pred)\n",
    "    print(\"Accuracy train :\",model,\" is: \",round(acc_score_train *100,2),\" %\")\n",
    "    print(\"Accuracy test :\",model,\" is: \",round(acc_score_test*100,2),\" %\")\n",
    "    print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d947a865",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m classifier_model()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier_model' is not defined"
     ]
    }
   ],
   "source": [
    "classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f9e0506",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[LogisticRegression(max_iter=1000000),LinearSVC(),RandomForestClassifier(),MLPClassifier(),GaussianNB(),DecisionTreeClassifier()]\n",
    "\n",
    "def classifier_model_with_cv():\n",
    "\n",
    "  for model in model_list:\n",
    "    cv_score = cross_val_score(model,X,Y,cv=5)\n",
    "    mean_score=sum(cv_score)/len(cv_score)\n",
    "    mean_score=mean_score*100\n",
    "    mean_score=round(mean_score,2)\n",
    "    \n",
    "  \n",
    "    print(\"Accuracy test :\",model,\" is: \",mean_score)\n",
    "    print(\"*******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f638b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test : LogisticRegression(max_iter=1000000)  is:  92.44\n",
      "*******\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test : LinearSVC()  is:  56.03\n",
      "*******\n",
      "Accuracy test : RandomForestClassifier()  is:  92.74\n",
      "*******\n",
      "Accuracy test : MLPClassifier()  is:  52.72\n",
      "*******\n",
      "Accuracy test : GaussianNB()  is:  85.38\n",
      "*******\n",
      "Accuracy test : DecisionTreeClassifier()  is:  89.62\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "classifier_model_with_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26050ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.1581347 , 0.84624338, 1.64474115, 0.15918627, 0.77198186,\n",
      "       1.63458595, 0.16082845, 0.77074652, 1.54187608]), 'std_fit_time': array([0.01821821, 0.04110863, 0.06166031, 0.00618273, 0.03134182,\n",
      "       0.1051349 , 0.0117498 , 0.04157818, 0.0577811 ]), 'mean_score_time': array([0.00765905, 0.01645141, 0.02974553, 0.00766706, 0.01990886,\n",
      "       0.01860461, 0.00336461, 0.01752481, 0.03382425]), 'std_score_time': array([0.00699182, 0.00115396, 0.00534613, 0.00110468, 0.00401078,\n",
      "       0.00512449, 0.00412122, 0.00380666, 0.00372718]), 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'entropy', 'entropy',\n",
      "                   'entropy', 'log_loss', 'log_loss', 'log_loss'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[10, 50, 100, 10, 50, 100, 10, 50, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'criterion': 'gini', 'n_estimators': 10}, {'criterion': 'gini', 'n_estimators': 50}, {'criterion': 'gini', 'n_estimators': 100}, {'criterion': 'entropy', 'n_estimators': 10}, {'criterion': 'entropy', 'n_estimators': 50}, {'criterion': 'entropy', 'n_estimators': 100}, {'criterion': 'log_loss', 'n_estimators': 10}, {'criterion': 'log_loss', 'n_estimators': 50}, {'criterion': 'log_loss', 'n_estimators': 100}], 'split0_test_score': array([0.93467337, 0.94472362, 0.94974874, 0.95477387, 0.93467337,\n",
      "       0.93467337, 0.94472362, 0.94974874, 0.94974874]), 'split1_test_score': array([0.91457286, 0.93969849, 0.92964824, 0.92462312, 0.92964824,\n",
      "       0.91959799, 0.92462312, 0.91457286, 0.93467337]), 'split2_test_score': array([0.9040404 , 0.92424242, 0.91919192, 0.91919192, 0.91414141,\n",
      "       0.9040404 , 0.9040404 , 0.91414141, 0.91919192]), 'split3_test_score': array([0.93434343, 0.96464646, 0.93939394, 0.94444444, 0.94949495,\n",
      "       0.93939394, 0.94949495, 0.94444444, 0.94444444]), 'split4_test_score': array([0.9040404 , 0.91919192, 0.91919192, 0.91414141, 0.8989899 ,\n",
      "       0.91414141, 0.9040404 , 0.90909091, 0.9040404 ]), 'mean_test_score': array([0.91833409, 0.93850058, 0.93143495, 0.93143495, 0.92538957,\n",
      "       0.92236942, 0.9253845 , 0.92639968, 0.93041978]), 'std_test_score': array([0.01375527, 0.01612382, 0.01184666, 0.01555192, 0.01737158,\n",
      "       0.01305799, 0.01932375, 0.01709108, 0.01679482]), 'rank_test_score': array([9, 1, 2, 2, 6, 8, 7, 5, 4])}\n",
      "0.9385005837267144\n"
     ]
    }
   ],
   "source": [
    "#hyperparamter tuning\n",
    "parameters={\n",
    "    'n_estimators':[10,50,100],\n",
    "    'criterion':[\"gini\", \"entropy\", \"log_loss\"]\n",
    "\n",
    "}\n",
    "modelX=RandomForestClassifier()\n",
    "classifier=  GridSearchCV(modelX,parameters,cv=5)\n",
    "classifier.fit(X,Y)\n",
    "print(classifier.cv_results_)\n",
    "print(classifier.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final classifier selected - RandomFoerest Classifier - It gave 93.85% accuracy on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bace3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(classifier.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "22139ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train : RandomForestClassifier(n_estimators=50)  is:  99.87  %\n",
      "Accuracy test : RandomForestClassifier(n_estimators=50)  is:  93.47  %\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "final_model=RandomForestClassifier(criterion=\"gini\",n_estimators=50)\n",
    "final_model.fit(X_train,Y_train)\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "acc_score_train = accuracy_score(Y_train,y_train_pred)\n",
    "acc_score_test = accuracy_score(Y_test,y_test_pred)\n",
    "print(\"Accuracy train :\",final_model,\" is: \",round(acc_score_train *100,2),\" %\")\n",
    "print(\"Accuracy test :\",final_model,\" is: \",round(acc_score_test*100,2),\" %\")\n",
    "print(\"**************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "99894825",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data2=(1,-1.35835406159823,-1.34016307473609,1.77320934263119,0.379779593034328,-0.503198133318193,1.80049938079263,0.791460956450422,0.247675786588991,-1.51465432260583,0.207642865216696,0.624501459424895,0.066083685268831,0.717292731410831,-0.165945922763554,2.34586494901581,-2.89008319444231,1.10996937869599,-0.121359313195888,-2.26185709530414,0.524979725224404,0.247998153469754,0.771679401917229,0.909412262347719,-0.689280956490685,-0.327641833735251,-0.139096571514147,-0.0553527940384261,-0.0597518405929204,378.66,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fea67e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# input_data = (406,-2.312226542,1.951992011,-1.609850732,3.997905588,-0.522187865,-1.426545319,-2.537387306,1.391657248,-2.770089277,-2.772272145,3.202033207,-2.899907388,-0.595221881,-4.289253782,0.38972412,-1.14074718,-2.830055675,-0.016822468,0.416955705,0.126910559,0.517232371,-0.035049369,-0.465211076,0.32019819,0.044519167,0.177839798,0.261145003,-0.143275875,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2048879b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00 -1.35835406e+00 -1.34016307e+00  1.77320934e+00\n",
      "  3.79779593e-01 -5.03198133e-01  1.80049938e+00  7.91460956e-01\n",
      "  2.47675787e-01 -1.51465432e+00  2.07642865e-01  6.24501459e-01\n",
      "  6.60836853e-02  7.17292731e-01 -1.65945923e-01  2.34586495e+00\n",
      " -2.89008319e+00  1.10996938e+00 -1.21359313e-01 -2.26185710e+00\n",
      "  5.24979725e-01  2.47998153e-01  7.71679402e-01  9.09412262e-01\n",
      " -6.89280956e-01 -3.27641834e-01 -1.39096572e-01 -5.53527940e-02\n",
      " -5.97518406e-02  3.78660000e+02]\n",
      "[[ 1.00000000e+00 -1.35835406e+00 -1.34016307e+00  1.77320934e+00\n",
      "   3.79779593e-01 -5.03198133e-01  1.80049938e+00  7.91460956e-01\n",
      "   2.47675787e-01 -1.51465432e+00  2.07642865e-01  6.24501459e-01\n",
      "   6.60836853e-02  7.17292731e-01 -1.65945923e-01  2.34586495e+00\n",
      "  -2.89008319e+00  1.10996938e+00 -1.21359313e-01 -2.26185710e+00\n",
      "   5.24979725e-01  2.47998153e-01  7.71679402e-01  9.09412262e-01\n",
      "  -6.89280956e-01 -3.27641834e-01 -1.39096572e-01 -5.53527940e-02\n",
      "  -5.97518406e-02  3.78660000e+02]]\n"
     ]
    }
   ],
   "source": [
    "input_array = np.asarray(input_data2)\n",
    "print(input_array)\n",
    "input_array=input_array.reshape(1,-1)\n",
    "print(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ac0c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is Safe!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91745\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# )\n",
    "input_df=pd.DataFrame(input_array.reshape(1,-1))\n",
    "# print(input_df)\n",
    "y_predd = final_model.predict(input_df)\n",
    "# print(y_predd)\n",
    "if(y_predd==1):\n",
    "    print(\"It is Fraud X\")\n",
    "else:\n",
    "    print(\"It is Safe!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447818c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2cf0606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout,Flatten\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fbc02b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_48 (Dense)            (None, 16)                496       \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 8)                 72        \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 785 (3.07 KB)\n",
      "Trainable params: 785 (3.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Dense(16,activation='relu',input_shape=(30,)))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "795775c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "363fa906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/350\n",
      "23/23 [==============================] - 2s 20ms/step - loss: 0.6313 - accuracy: 0.7574 - val_loss: 0.6096 - val_accuracy: 0.7625\n",
      "Epoch 2/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.5499 - accuracy: 0.7938 - val_loss: 0.5429 - val_accuracy: 0.7625\n",
      "Epoch 3/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.4604 - accuracy: 0.8485 - val_loss: 0.4563 - val_accuracy: 0.8250\n",
      "Epoch 4/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3711 - accuracy: 0.8878 - val_loss: 0.3944 - val_accuracy: 0.8375\n",
      "Epoch 5/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.3098 - accuracy: 0.9018 - val_loss: 0.3530 - val_accuracy: 0.8375\n",
      "Epoch 6/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2678 - accuracy: 0.9102 - val_loss: 0.3272 - val_accuracy: 0.8500\n",
      "Epoch 7/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2367 - accuracy: 0.9187 - val_loss: 0.2976 - val_accuracy: 0.8750\n",
      "Epoch 8/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.2116 - accuracy: 0.9355 - val_loss: 0.2842 - val_accuracy: 0.8875\n",
      "Epoch 9/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1915 - accuracy: 0.9369 - val_loss: 0.2638 - val_accuracy: 0.9000\n",
      "Epoch 10/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1736 - accuracy: 0.9439 - val_loss: 0.2486 - val_accuracy: 0.9000\n",
      "Epoch 11/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1610 - accuracy: 0.9453 - val_loss: 0.2383 - val_accuracy: 0.9125\n",
      "Epoch 12/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1496 - accuracy: 0.9495 - val_loss: 0.2293 - val_accuracy: 0.9125\n",
      "Epoch 13/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1444 - accuracy: 0.9523 - val_loss: 0.2217 - val_accuracy: 0.9250\n",
      "Epoch 14/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9537 - val_loss: 0.2114 - val_accuracy: 0.9250\n",
      "Epoch 15/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1273 - accuracy: 0.9593 - val_loss: 0.2061 - val_accuracy: 0.9250\n",
      "Epoch 16/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1212 - accuracy: 0.9635 - val_loss: 0.1937 - val_accuracy: 0.9250\n",
      "Epoch 17/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1168 - accuracy: 0.9649 - val_loss: 0.1920 - val_accuracy: 0.9250\n",
      "Epoch 18/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1100 - accuracy: 0.9663 - val_loss: 0.1893 - val_accuracy: 0.9250\n",
      "Epoch 19/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1076 - accuracy: 0.9691 - val_loss: 0.1908 - val_accuracy: 0.9250\n",
      "Epoch 20/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1031 - accuracy: 0.9677 - val_loss: 0.1720 - val_accuracy: 0.9250\n",
      "Epoch 21/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1039 - accuracy: 0.9635 - val_loss: 0.1963 - val_accuracy: 0.9250\n",
      "Epoch 22/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1016 - accuracy: 0.9635 - val_loss: 0.1691 - val_accuracy: 0.9125\n",
      "Epoch 23/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0959 - accuracy: 0.9705 - val_loss: 0.1734 - val_accuracy: 0.9250\n",
      "Epoch 24/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0924 - accuracy: 0.9748 - val_loss: 0.1613 - val_accuracy: 0.9250\n",
      "Epoch 25/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0903 - accuracy: 0.9734 - val_loss: 0.1625 - val_accuracy: 0.9250\n",
      "Epoch 26/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0880 - accuracy: 0.9734 - val_loss: 0.1558 - val_accuracy: 0.9250\n",
      "Epoch 27/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9748 - val_loss: 0.1521 - val_accuracy: 0.9250\n",
      "Epoch 28/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0856 - accuracy: 0.9776 - val_loss: 0.1629 - val_accuracy: 0.9250\n",
      "Epoch 29/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0848 - accuracy: 0.9762 - val_loss: 0.1527 - val_accuracy: 0.9250\n",
      "Epoch 30/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0821 - accuracy: 0.9748 - val_loss: 0.1414 - val_accuracy: 0.9250\n",
      "Epoch 31/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0800 - accuracy: 0.9776 - val_loss: 0.1417 - val_accuracy: 0.9250\n",
      "Epoch 32/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0787 - accuracy: 0.9776 - val_loss: 0.1453 - val_accuracy: 0.9250\n",
      "Epoch 33/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0768 - accuracy: 0.9776 - val_loss: 0.1388 - val_accuracy: 0.9500\n",
      "Epoch 34/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0760 - accuracy: 0.9748 - val_loss: 0.1523 - val_accuracy: 0.9375\n",
      "Epoch 35/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0747 - accuracy: 0.9790 - val_loss: 0.1346 - val_accuracy: 0.9500\n",
      "Epoch 36/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9790 - val_loss: 0.1416 - val_accuracy: 0.9250\n",
      "Epoch 37/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0718 - accuracy: 0.9818 - val_loss: 0.1349 - val_accuracy: 0.9375\n",
      "Epoch 38/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0696 - accuracy: 0.9818 - val_loss: 0.1312 - val_accuracy: 0.9375\n",
      "Epoch 39/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0694 - accuracy: 0.9790 - val_loss: 0.1358 - val_accuracy: 0.9500\n",
      "Epoch 40/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9818 - val_loss: 0.1222 - val_accuracy: 0.9625\n",
      "Epoch 41/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9734 - val_loss: 0.1285 - val_accuracy: 0.9625\n",
      "Epoch 42/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0661 - accuracy: 0.9790 - val_loss: 0.1263 - val_accuracy: 0.9500\n",
      "Epoch 43/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0643 - accuracy: 0.9818 - val_loss: 0.1283 - val_accuracy: 0.9625\n",
      "Epoch 44/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0626 - accuracy: 0.9790 - val_loss: 0.1215 - val_accuracy: 0.9750\n",
      "Epoch 45/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9832 - val_loss: 0.1184 - val_accuracy: 0.9750\n",
      "Epoch 46/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9790 - val_loss: 0.1230 - val_accuracy: 0.9750\n",
      "Epoch 47/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9790 - val_loss: 0.1153 - val_accuracy: 0.9750\n",
      "Epoch 48/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0571 - accuracy: 0.9832 - val_loss: 0.1190 - val_accuracy: 0.9750\n",
      "Epoch 49/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0567 - accuracy: 0.9832 - val_loss: 0.1131 - val_accuracy: 0.9750\n",
      "Epoch 50/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0555 - accuracy: 0.9804 - val_loss: 0.1114 - val_accuracy: 0.9750\n",
      "Epoch 51/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0535 - accuracy: 0.9846 - val_loss: 0.1103 - val_accuracy: 0.9750\n",
      "Epoch 52/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0526 - accuracy: 0.9874 - val_loss: 0.1103 - val_accuracy: 0.9750\n",
      "Epoch 53/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0565 - accuracy: 0.9804 - val_loss: 0.1057 - val_accuracy: 0.9625\n",
      "Epoch 54/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0510 - accuracy: 0.9860 - val_loss: 0.1095 - val_accuracy: 0.9750\n",
      "Epoch 55/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0506 - accuracy: 0.9818 - val_loss: 0.1082 - val_accuracy: 0.9750\n",
      "Epoch 56/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0491 - accuracy: 0.9832 - val_loss: 0.1058 - val_accuracy: 0.9750\n",
      "Epoch 57/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0470 - accuracy: 0.9874 - val_loss: 0.1055 - val_accuracy: 0.9750\n",
      "Epoch 58/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0468 - accuracy: 0.9888 - val_loss: 0.1040 - val_accuracy: 0.9750\n",
      "Epoch 59/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0453 - accuracy: 0.9832 - val_loss: 0.1068 - val_accuracy: 0.9750\n",
      "Epoch 60/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9832 - val_loss: 0.1094 - val_accuracy: 0.9750\n",
      "Epoch 61/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0473 - accuracy: 0.9902 - val_loss: 0.1208 - val_accuracy: 0.9750\n",
      "Epoch 62/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0441 - accuracy: 0.9930 - val_loss: 0.1188 - val_accuracy: 0.9625\n",
      "Epoch 63/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0432 - accuracy: 0.9902 - val_loss: 0.1150 - val_accuracy: 0.9750\n",
      "Epoch 64/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9874 - val_loss: 0.1025 - val_accuracy: 0.9625\n",
      "Epoch 65/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0411 - accuracy: 0.9930 - val_loss: 0.1074 - val_accuracy: 0.9625\n",
      "Epoch 66/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0419 - accuracy: 0.9888 - val_loss: 0.1062 - val_accuracy: 0.9625\n",
      "Epoch 67/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0396 - accuracy: 0.9916 - val_loss: 0.1086 - val_accuracy: 0.9750\n",
      "Epoch 68/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0386 - accuracy: 0.9930 - val_loss: 0.1063 - val_accuracy: 0.9625\n",
      "Epoch 69/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.9930 - val_loss: 0.1159 - val_accuracy: 0.9625\n",
      "Epoch 70/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0379 - accuracy: 0.9902 - val_loss: 0.1062 - val_accuracy: 0.9625\n",
      "Epoch 71/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0371 - accuracy: 0.9930 - val_loss: 0.0999 - val_accuracy: 0.9625\n",
      "Epoch 72/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0364 - accuracy: 0.9930 - val_loss: 0.1047 - val_accuracy: 0.9750\n",
      "Epoch 73/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0344 - accuracy: 0.9930 - val_loss: 0.1032 - val_accuracy: 0.9625\n",
      "Epoch 74/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0341 - accuracy: 0.9944 - val_loss: 0.1039 - val_accuracy: 0.9625\n",
      "Epoch 75/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0334 - accuracy: 0.9930 - val_loss: 0.1015 - val_accuracy: 0.9625\n",
      "Epoch 76/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0323 - accuracy: 0.9916 - val_loss: 0.1153 - val_accuracy: 0.9625\n",
      "Epoch 77/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0312 - accuracy: 0.9944 - val_loss: 0.1063 - val_accuracy: 0.9625\n",
      "Epoch 78/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0315 - accuracy: 0.9916 - val_loss: 0.1088 - val_accuracy: 0.9625\n",
      "Epoch 79/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0347 - accuracy: 0.9930 - val_loss: 0.1023 - val_accuracy: 0.9500\n",
      "Epoch 80/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0332 - accuracy: 0.9902 - val_loss: 0.1101 - val_accuracy: 0.9625\n",
      "Epoch 81/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0292 - accuracy: 0.9958 - val_loss: 0.1162 - val_accuracy: 0.9625\n",
      "Epoch 82/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0284 - accuracy: 0.9944 - val_loss: 0.1110 - val_accuracy: 0.9625\n",
      "Epoch 83/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0285 - accuracy: 0.9958 - val_loss: 0.1290 - val_accuracy: 0.9625\n",
      "Epoch 84/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0274 - accuracy: 0.9944 - val_loss: 0.1104 - val_accuracy: 0.9500\n",
      "Epoch 85/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0262 - accuracy: 0.9958 - val_loss: 0.1218 - val_accuracy: 0.9625\n",
      "Epoch 86/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0262 - accuracy: 0.9958 - val_loss: 0.1188 - val_accuracy: 0.9625\n",
      "Epoch 87/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0255 - accuracy: 0.9958 - val_loss: 0.1201 - val_accuracy: 0.9625\n",
      "Epoch 88/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0254 - accuracy: 0.9958 - val_loss: 0.1230 - val_accuracy: 0.9625\n",
      "Epoch 89/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0250 - accuracy: 0.9958 - val_loss: 0.1212 - val_accuracy: 0.9625\n",
      "Epoch 90/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0245 - accuracy: 0.9958 - val_loss: 0.1272 - val_accuracy: 0.9625\n",
      "Epoch 91/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0246 - accuracy: 0.9958 - val_loss: 0.1127 - val_accuracy: 0.9625\n",
      "Epoch 92/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0256 - accuracy: 0.9944 - val_loss: 0.1361 - val_accuracy: 0.9750\n",
      "Epoch 93/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0238 - accuracy: 0.9958 - val_loss: 0.1275 - val_accuracy: 0.9625\n",
      "Epoch 94/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 0.0230 - accuracy: 0.9958 - val_loss: 0.1203 - val_accuracy: 0.9625\n",
      "Epoch 95/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0228 - accuracy: 0.9958 - val_loss: 0.1268 - val_accuracy: 0.9625\n",
      "Epoch 96/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0252 - accuracy: 0.9958 - val_loss: 0.1374 - val_accuracy: 0.9750\n",
      "Epoch 97/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0222 - accuracy: 0.9958 - val_loss: 0.1288 - val_accuracy: 0.9625\n",
      "Epoch 98/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0215 - accuracy: 0.9958 - val_loss: 0.1315 - val_accuracy: 0.9625\n",
      "Epoch 99/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0205 - accuracy: 0.9958 - val_loss: 0.1312 - val_accuracy: 0.9625\n",
      "Epoch 100/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0198 - accuracy: 0.9958 - val_loss: 0.1378 - val_accuracy: 0.9625\n",
      "Epoch 101/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0204 - accuracy: 0.9958 - val_loss: 0.1298 - val_accuracy: 0.9625\n",
      "Epoch 102/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0193 - accuracy: 0.9958 - val_loss: 0.1372 - val_accuracy: 0.9625\n",
      "Epoch 103/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0191 - accuracy: 0.9972 - val_loss: 0.1345 - val_accuracy: 0.9625\n",
      "Epoch 104/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0190 - accuracy: 0.9958 - val_loss: 0.1387 - val_accuracy: 0.9625\n",
      "Epoch 105/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0196 - accuracy: 0.9986 - val_loss: 0.1527 - val_accuracy: 0.9750\n",
      "Epoch 106/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0197 - accuracy: 0.9958 - val_loss: 0.1405 - val_accuracy: 0.9625\n",
      "Epoch 107/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0181 - accuracy: 0.9986 - val_loss: 0.1470 - val_accuracy: 0.9750\n",
      "Epoch 108/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0183 - accuracy: 0.9972 - val_loss: 0.1370 - val_accuracy: 0.9625\n",
      "Epoch 109/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0169 - accuracy: 0.9972 - val_loss: 0.1389 - val_accuracy: 0.9625\n",
      "Epoch 110/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0170 - accuracy: 0.9958 - val_loss: 0.1375 - val_accuracy: 0.9500\n",
      "Epoch 111/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0176 - accuracy: 0.9986 - val_loss: 0.1447 - val_accuracy: 0.9625\n",
      "Epoch 112/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0166 - accuracy: 0.9972 - val_loss: 0.1473 - val_accuracy: 0.9625\n",
      "Epoch 113/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0177 - accuracy: 0.9972 - val_loss: 0.1410 - val_accuracy: 0.9500\n",
      "Epoch 114/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0157 - accuracy: 0.9986 - val_loss: 0.1575 - val_accuracy: 0.9750\n",
      "Epoch 115/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0167 - accuracy: 0.9986 - val_loss: 0.1464 - val_accuracy: 0.9500\n",
      "Epoch 116/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0158 - accuracy: 0.9972 - val_loss: 0.1432 - val_accuracy: 0.9500\n",
      "Epoch 117/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9986 - val_loss: 0.1497 - val_accuracy: 0.9500\n",
      "Epoch 118/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0154 - accuracy: 0.9972 - val_loss: 0.1470 - val_accuracy: 0.9500\n",
      "Epoch 119/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0153 - accuracy: 0.9986 - val_loss: 0.1652 - val_accuracy: 0.9625\n",
      "Epoch 120/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0173 - accuracy: 0.9972 - val_loss: 0.1514 - val_accuracy: 0.9500\n",
      "Epoch 121/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0156 - accuracy: 0.9972 - val_loss: 0.1609 - val_accuracy: 0.9500\n",
      "Epoch 122/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0162 - accuracy: 0.9972 - val_loss: 0.1519 - val_accuracy: 0.9750\n",
      "Epoch 123/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0146 - accuracy: 0.9986 - val_loss: 0.1536 - val_accuracy: 0.9625\n",
      "Epoch 124/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0164 - accuracy: 0.9972 - val_loss: 0.1492 - val_accuracy: 0.9750\n",
      "Epoch 125/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0157 - accuracy: 0.9972 - val_loss: 0.1505 - val_accuracy: 0.9625\n",
      "Epoch 126/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0140 - accuracy: 0.9986 - val_loss: 0.1546 - val_accuracy: 0.9500\n",
      "Epoch 127/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0137 - accuracy: 0.9972 - val_loss: 0.1523 - val_accuracy: 0.9625\n",
      "Epoch 128/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0127 - accuracy: 0.9986 - val_loss: 0.1555 - val_accuracy: 0.9625\n",
      "Epoch 129/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0128 - accuracy: 0.9986 - val_loss: 0.1532 - val_accuracy: 0.9750\n",
      "Epoch 130/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0122 - accuracy: 0.9986 - val_loss: 0.1548 - val_accuracy: 0.9625\n",
      "Epoch 131/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0132 - accuracy: 0.9986 - val_loss: 0.1623 - val_accuracy: 0.9750\n",
      "Epoch 132/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.1655 - val_accuracy: 0.9500\n",
      "Epoch 133/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0124 - accuracy: 0.9986 - val_loss: 0.1551 - val_accuracy: 0.9750\n",
      "Epoch 134/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0120 - accuracy: 0.9986 - val_loss: 0.1626 - val_accuracy: 0.9625\n",
      "Epoch 135/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.9986 - val_loss: 0.1447 - val_accuracy: 0.9750\n",
      "Epoch 136/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0117 - accuracy: 0.9986 - val_loss: 0.1529 - val_accuracy: 0.9625\n",
      "Epoch 137/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0112 - accuracy: 0.9986 - val_loss: 0.1554 - val_accuracy: 0.9625\n",
      "Epoch 138/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0110 - accuracy: 0.9986 - val_loss: 0.1559 - val_accuracy: 0.9625\n",
      "Epoch 139/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.1638 - val_accuracy: 0.9500\n",
      "Epoch 140/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.1639 - val_accuracy: 0.9625\n",
      "Epoch 141/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.1598 - val_accuracy: 0.9625\n",
      "Epoch 142/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0103 - accuracy: 0.9986 - val_loss: 0.1628 - val_accuracy: 0.9625\n",
      "Epoch 143/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0108 - accuracy: 0.9986 - val_loss: 0.1674 - val_accuracy: 0.9625\n",
      "Epoch 144/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 0.1598 - val_accuracy: 0.9750\n",
      "Epoch 145/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0100 - accuracy: 0.9986 - val_loss: 0.1647 - val_accuracy: 0.9625\n",
      "Epoch 146/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 0.1708 - val_accuracy: 0.9625\n",
      "Epoch 147/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9986 - val_loss: 0.1656 - val_accuracy: 0.9625\n",
      "Epoch 148/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0095 - accuracy: 0.9986 - val_loss: 0.1705 - val_accuracy: 0.9500\n",
      "Epoch 149/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 0.1686 - val_accuracy: 0.9625\n",
      "Epoch 150/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0094 - accuracy: 0.9986 - val_loss: 0.1669 - val_accuracy: 0.9625\n",
      "Epoch 151/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0092 - accuracy: 0.9986 - val_loss: 0.1723 - val_accuracy: 0.9625\n",
      "Epoch 152/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0090 - accuracy: 0.9986 - val_loss: 0.1697 - val_accuracy: 0.9625\n",
      "Epoch 153/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.1697 - val_accuracy: 0.9625\n",
      "Epoch 154/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0088 - accuracy: 0.9986 - val_loss: 0.1694 - val_accuracy: 0.9625\n",
      "Epoch 155/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.1751 - val_accuracy: 0.9625\n",
      "Epoch 156/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.1818 - val_accuracy: 0.9500\n",
      "Epoch 157/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 0.9986 - val_loss: 0.1785 - val_accuracy: 0.9625\n",
      "Epoch 158/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 0.9986 - val_loss: 0.1879 - val_accuracy: 0.9500\n",
      "Epoch 159/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.1857 - val_accuracy: 0.9500\n",
      "Epoch 160/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0083 - accuracy: 0.9986 - val_loss: 0.1866 - val_accuracy: 0.9500\n",
      "Epoch 161/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.1808 - val_accuracy: 0.9625\n",
      "Epoch 162/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9986 - val_loss: 0.1842 - val_accuracy: 0.9625\n",
      "Epoch 163/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0080 - accuracy: 0.9986 - val_loss: 0.1903 - val_accuracy: 0.9500\n",
      "Epoch 164/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0076 - accuracy: 0.9986 - val_loss: 0.1845 - val_accuracy: 0.9625\n",
      "Epoch 165/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.1978 - val_accuracy: 0.9500\n",
      "Epoch 166/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 0.1842 - val_accuracy: 0.9625\n",
      "Epoch 167/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 0.9986 - val_loss: 0.1972 - val_accuracy: 0.9500\n",
      "Epoch 168/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0079 - accuracy: 0.9986 - val_loss: 0.2165 - val_accuracy: 0.9500\n",
      "Epoch 169/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.2013 - val_accuracy: 0.9500\n",
      "Epoch 170/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0082 - accuracy: 0.9986 - val_loss: 0.2293 - val_accuracy: 0.9625\n",
      "Epoch 171/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.2163 - val_accuracy: 0.9625\n",
      "Epoch 172/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.2310 - val_accuracy: 0.9375\n",
      "Epoch 173/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0074 - accuracy: 0.9986 - val_loss: 0.2228 - val_accuracy: 0.9625\n",
      "Epoch 174/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.2200 - val_accuracy: 0.9500\n",
      "Epoch 175/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.2301 - val_accuracy: 0.9500\n",
      "Epoch 176/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.2303 - val_accuracy: 0.9500\n",
      "Epoch 177/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.2386 - val_accuracy: 0.9500\n",
      "Epoch 178/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0100 - accuracy: 0.9958 - val_loss: 0.2614 - val_accuracy: 0.9500\n",
      "Epoch 179/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.2492 - val_accuracy: 0.9375\n",
      "Epoch 180/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.9625\n",
      "Epoch 181/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.2298 - val_accuracy: 0.9625\n",
      "Epoch 182/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.2278 - val_accuracy: 0.9625\n",
      "Epoch 183/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9500\n",
      "Epoch 184/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.2415 - val_accuracy: 0.9500\n",
      "Epoch 185/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2428 - val_accuracy: 0.9500\n",
      "Epoch 186/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9500\n",
      "Epoch 187/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2462 - val_accuracy: 0.9500\n",
      "Epoch 188/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2470 - val_accuracy: 0.9500\n",
      "Epoch 189/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9500\n",
      "Epoch 190/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2534 - val_accuracy: 0.9625\n",
      "Epoch 191/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2519 - val_accuracy: 0.9500\n",
      "Epoch 192/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2542 - val_accuracy: 0.9500\n",
      "Epoch 193/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.9033e-04 - accuracy: 1.0000 - val_loss: 0.2553 - val_accuracy: 0.9500\n",
      "Epoch 194/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.5995e-04 - accuracy: 1.0000 - val_loss: 0.2563 - val_accuracy: 0.9500\n",
      "Epoch 195/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.5384e-04 - accuracy: 1.0000 - val_loss: 0.2587 - val_accuracy: 0.9500\n",
      "Epoch 196/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 9.3414e-04 - accuracy: 1.0000 - val_loss: 0.2595 - val_accuracy: 0.9500\n",
      "Epoch 197/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 9.3666e-04 - accuracy: 1.0000 - val_loss: 0.2584 - val_accuracy: 0.9500\n",
      "Epoch 198/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 8.6238e-04 - accuracy: 1.0000 - val_loss: 0.2628 - val_accuracy: 0.9500\n",
      "Epoch 199/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 8.3264e-04 - accuracy: 1.0000 - val_loss: 0.2647 - val_accuracy: 0.9500\n",
      "Epoch 200/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 7.8877e-04 - accuracy: 1.0000 - val_loss: 0.2649 - val_accuracy: 0.9500\n",
      "Epoch 201/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 9.4046e-04 - accuracy: 1.0000 - val_loss: 0.2643 - val_accuracy: 0.9500\n",
      "Epoch 202/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 8.1776e-04 - accuracy: 1.0000 - val_loss: 0.2681 - val_accuracy: 0.9500\n",
      "Epoch 203/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.7123e-04 - accuracy: 1.0000 - val_loss: 0.2676 - val_accuracy: 0.9500\n",
      "Epoch 204/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 7.1814e-04 - accuracy: 1.0000 - val_loss: 0.2673 - val_accuracy: 0.9500\n",
      "Epoch 205/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 6.6563e-04 - accuracy: 1.0000 - val_loss: 0.2703 - val_accuracy: 0.9500\n",
      "Epoch 206/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.4963e-04 - accuracy: 1.0000 - val_loss: 0.2707 - val_accuracy: 0.9500\n",
      "Epoch 207/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.5029e-04 - accuracy: 1.0000 - val_loss: 0.2710 - val_accuracy: 0.9500\n",
      "Epoch 208/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 6.3356e-04 - accuracy: 1.0000 - val_loss: 0.2726 - val_accuracy: 0.9500\n",
      "Epoch 209/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.0781e-04 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9500\n",
      "Epoch 210/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 5.8378e-04 - accuracy: 1.0000 - val_loss: 0.2755 - val_accuracy: 0.9500\n",
      "Epoch 211/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.3734e-04 - accuracy: 1.0000 - val_loss: 0.2768 - val_accuracy: 0.9500\n",
      "Epoch 212/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 5.5469e-04 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9500\n",
      "Epoch 213/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 5.4367e-04 - accuracy: 1.0000 - val_loss: 0.2781 - val_accuracy: 0.9500\n",
      "Epoch 214/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 5.3262e-04 - accuracy: 1.0000 - val_loss: 0.2786 - val_accuracy: 0.9500\n",
      "Epoch 215/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 5.1869e-04 - accuracy: 1.0000 - val_loss: 0.2811 - val_accuracy: 0.9500\n",
      "Epoch 216/350\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 5.0895e-04 - accuracy: 1.0000 - val_loss: 0.2794 - val_accuracy: 0.9500\n",
      "Epoch 217/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4.8625e-04 - accuracy: 1.0000 - val_loss: 0.2813 - val_accuracy: 0.9500\n",
      "Epoch 218/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.9576e-04 - accuracy: 1.0000 - val_loss: 0.2830 - val_accuracy: 0.9500\n",
      "Epoch 219/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.7410e-04 - accuracy: 1.0000 - val_loss: 0.2825 - val_accuracy: 0.9500\n",
      "Epoch 220/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4.6151e-04 - accuracy: 1.0000 - val_loss: 0.2833 - val_accuracy: 0.9500\n",
      "Epoch 221/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.5720e-04 - accuracy: 1.0000 - val_loss: 0.2858 - val_accuracy: 0.9500\n",
      "Epoch 222/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.3655e-04 - accuracy: 1.0000 - val_loss: 0.2867 - val_accuracy: 0.9500\n",
      "Epoch 223/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.1896e-04 - accuracy: 1.0000 - val_loss: 0.2858 - val_accuracy: 0.9500\n",
      "Epoch 224/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.1215e-04 - accuracy: 1.0000 - val_loss: 0.2874 - val_accuracy: 0.9500\n",
      "Epoch 225/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.0520e-04 - accuracy: 1.0000 - val_loss: 0.2883 - val_accuracy: 0.9500\n",
      "Epoch 226/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2800e-04 - accuracy: 1.0000 - val_loss: 0.2922 - val_accuracy: 0.9375\n",
      "Epoch 227/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.1469e-04 - accuracy: 1.0000 - val_loss: 0.2905 - val_accuracy: 0.9500\n",
      "Epoch 228/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3.8563e-04 - accuracy: 1.0000 - val_loss: 0.2915 - val_accuracy: 0.9500\n",
      "Epoch 229/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.7963e-04 - accuracy: 1.0000 - val_loss: 0.2923 - val_accuracy: 0.9500\n",
      "Epoch 230/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.7264e-04 - accuracy: 1.0000 - val_loss: 0.2923 - val_accuracy: 0.9500\n",
      "Epoch 231/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.7722e-04 - accuracy: 1.0000 - val_loss: 0.2944 - val_accuracy: 0.9500\n",
      "Epoch 232/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.6532e-04 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.9500\n",
      "Epoch 233/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.3952e-04 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9500\n",
      "Epoch 234/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 3.3472e-04 - accuracy: 1.0000 - val_loss: 0.2968 - val_accuracy: 0.9500\n",
      "Epoch 235/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.2440e-04 - accuracy: 1.0000 - val_loss: 0.2969 - val_accuracy: 0.9500\n",
      "Epoch 236/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.4178e-04 - accuracy: 1.0000 - val_loss: 0.2985 - val_accuracy: 0.9500\n",
      "Epoch 237/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.3098e-04 - accuracy: 1.0000 - val_loss: 0.2993 - val_accuracy: 0.9500\n",
      "Epoch 238/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.2598e-04 - accuracy: 1.0000 - val_loss: 0.2995 - val_accuracy: 0.9500\n",
      "Epoch 239/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.0771e-04 - accuracy: 1.0000 - val_loss: 0.3000 - val_accuracy: 0.9500\n",
      "Epoch 240/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.9875e-04 - accuracy: 1.0000 - val_loss: 0.3016 - val_accuracy: 0.9500\n",
      "Epoch 241/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.9538e-04 - accuracy: 1.0000 - val_loss: 0.3016 - val_accuracy: 0.9500\n",
      "Epoch 242/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.8219e-04 - accuracy: 1.0000 - val_loss: 0.3028 - val_accuracy: 0.9500\n",
      "Epoch 243/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.9050e-04 - accuracy: 1.0000 - val_loss: 0.3060 - val_accuracy: 0.9500\n",
      "Epoch 244/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.8883e-04 - accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9500\n",
      "Epoch 245/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.6906e-04 - accuracy: 1.0000 - val_loss: 0.3070 - val_accuracy: 0.9500\n",
      "Epoch 246/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.6334e-04 - accuracy: 1.0000 - val_loss: 0.3075 - val_accuracy: 0.9500\n",
      "Epoch 247/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.7632e-04 - accuracy: 1.0000 - val_loss: 0.3087 - val_accuracy: 0.9500\n",
      "Epoch 248/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.6352e-04 - accuracy: 1.0000 - val_loss: 0.3090 - val_accuracy: 0.9500\n",
      "Epoch 249/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.5389e-04 - accuracy: 1.0000 - val_loss: 0.3101 - val_accuracy: 0.9500\n",
      "Epoch 250/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.5206e-04 - accuracy: 1.0000 - val_loss: 0.3098 - val_accuracy: 0.9500\n",
      "Epoch 251/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.3651e-04 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.9500\n",
      "Epoch 252/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.3482e-04 - accuracy: 1.0000 - val_loss: 0.3122 - val_accuracy: 0.9500\n",
      "Epoch 253/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.2500e-04 - accuracy: 1.0000 - val_loss: 0.3119 - val_accuracy: 0.9500\n",
      "Epoch 254/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.3085e-04 - accuracy: 1.0000 - val_loss: 0.3125 - val_accuracy: 0.9500\n",
      "Epoch 255/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.2460e-04 - accuracy: 1.0000 - val_loss: 0.3157 - val_accuracy: 0.9500\n",
      "Epoch 256/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1421e-04 - accuracy: 1.0000 - val_loss: 0.3167 - val_accuracy: 0.9500\n",
      "Epoch 257/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1047e-04 - accuracy: 1.0000 - val_loss: 0.3162 - val_accuracy: 0.9500\n",
      "Epoch 258/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1284e-04 - accuracy: 1.0000 - val_loss: 0.3188 - val_accuracy: 0.9500\n",
      "Epoch 259/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1061e-04 - accuracy: 1.0000 - val_loss: 0.3201 - val_accuracy: 0.9500\n",
      "Epoch 260/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 2.0162e-04 - accuracy: 1.0000 - val_loss: 0.3193 - val_accuracy: 0.9500\n",
      "Epoch 261/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.9464e-04 - accuracy: 1.0000 - val_loss: 0.3201 - val_accuracy: 0.9500\n",
      "Epoch 262/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.8705e-04 - accuracy: 1.0000 - val_loss: 0.3209 - val_accuracy: 0.9500\n",
      "Epoch 263/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.8448e-04 - accuracy: 1.0000 - val_loss: 0.3215 - val_accuracy: 0.9500\n",
      "Epoch 264/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 1.8044e-04 - accuracy: 1.0000 - val_loss: 0.3227 - val_accuracy: 0.9500\n",
      "Epoch 265/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7645e-04 - accuracy: 1.0000 - val_loss: 0.3239 - val_accuracy: 0.9500\n",
      "Epoch 266/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.8200e-04 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 0.9500\n",
      "Epoch 267/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7524e-04 - accuracy: 1.0000 - val_loss: 0.3249 - val_accuracy: 0.9500\n",
      "Epoch 268/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6802e-04 - accuracy: 1.0000 - val_loss: 0.3277 - val_accuracy: 0.9500\n",
      "Epoch 269/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6561e-04 - accuracy: 1.0000 - val_loss: 0.3289 - val_accuracy: 0.9500\n",
      "Epoch 270/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6420e-04 - accuracy: 1.0000 - val_loss: 0.3297 - val_accuracy: 0.9500\n",
      "Epoch 271/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5715e-04 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 0.9500\n",
      "Epoch 272/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5442e-04 - accuracy: 1.0000 - val_loss: 0.3302 - val_accuracy: 0.9500\n",
      "Epoch 273/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.5257e-04 - accuracy: 1.0000 - val_loss: 0.3312 - val_accuracy: 0.9500\n",
      "Epoch 274/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4535e-04 - accuracy: 1.0000 - val_loss: 0.3314 - val_accuracy: 0.9500\n",
      "Epoch 275/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.5064e-04 - accuracy: 1.0000 - val_loss: 0.3336 - val_accuracy: 0.9500\n",
      "Epoch 276/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.4304e-04 - accuracy: 1.0000 - val_loss: 0.3334 - val_accuracy: 0.9500\n",
      "Epoch 277/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3910e-04 - accuracy: 1.0000 - val_loss: 0.3341 - val_accuracy: 0.9500\n",
      "Epoch 278/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3679e-04 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 0.9500\n",
      "Epoch 279/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3447e-04 - accuracy: 1.0000 - val_loss: 0.3359 - val_accuracy: 0.9500\n",
      "Epoch 280/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.3175e-04 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9500\n",
      "Epoch 281/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2945e-04 - accuracy: 1.0000 - val_loss: 0.3372 - val_accuracy: 0.9500\n",
      "Epoch 282/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.2735e-04 - accuracy: 1.0000 - val_loss: 0.3376 - val_accuracy: 0.9500\n",
      "Epoch 283/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2462e-04 - accuracy: 1.0000 - val_loss: 0.3387 - val_accuracy: 0.9500\n",
      "Epoch 284/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.2355e-04 - accuracy: 1.0000 - val_loss: 0.3397 - val_accuracy: 0.9500\n",
      "Epoch 285/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 1.1805e-04 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.9500\n",
      "Epoch 286/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1949e-04 - accuracy: 1.0000 - val_loss: 0.3408 - val_accuracy: 0.9500\n",
      "Epoch 287/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1673e-04 - accuracy: 1.0000 - val_loss: 0.3409 - val_accuracy: 0.9500\n",
      "Epoch 288/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1410e-04 - accuracy: 1.0000 - val_loss: 0.3411 - val_accuracy: 0.9500\n",
      "Epoch 289/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1567e-04 - accuracy: 1.0000 - val_loss: 0.3418 - val_accuracy: 0.9500\n",
      "Epoch 290/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.1087e-04 - accuracy: 1.0000 - val_loss: 0.3433 - val_accuracy: 0.9500\n",
      "Epoch 291/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1157e-04 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9500\n",
      "Epoch 292/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.0573e-04 - accuracy: 1.0000 - val_loss: 0.3440 - val_accuracy: 0.9500\n",
      "Epoch 293/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.0372e-04 - accuracy: 1.0000 - val_loss: 0.3454 - val_accuracy: 0.9500\n",
      "Epoch 294/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.0372e-04 - accuracy: 1.0000 - val_loss: 0.3472 - val_accuracy: 0.9500\n",
      "Epoch 295/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.0097e-04 - accuracy: 1.0000 - val_loss: 0.3471 - val_accuracy: 0.9500\n",
      "Epoch 296/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 9.8904e-05 - accuracy: 1.0000 - val_loss: 0.3473 - val_accuracy: 0.9500\n",
      "Epoch 297/350\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 9.8641e-05 - accuracy: 1.0000 - val_loss: 0.3479 - val_accuracy: 0.9500\n",
      "Epoch 298/350\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 9.7156e-05 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.9500\n",
      "Epoch 299/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 9.6845e-05 - accuracy: 1.0000 - val_loss: 0.3514 - val_accuracy: 0.9500\n",
      "Epoch 300/350\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 9.3726e-05 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 0.9500\n",
      "Epoch 301/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 8.9707e-05 - accuracy: 1.0000 - val_loss: 0.3503 - val_accuracy: 0.9500\n",
      "Epoch 302/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 8.9393e-05 - accuracy: 1.0000 - val_loss: 0.3506 - val_accuracy: 0.9500\n",
      "Epoch 303/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 9.1210e-05 - accuracy: 1.0000 - val_loss: 0.3533 - val_accuracy: 0.9500\n",
      "Epoch 304/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 9.4254e-05 - accuracy: 1.0000 - val_loss: 0.3523 - val_accuracy: 0.9500\n",
      "Epoch 305/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 8.5376e-05 - accuracy: 1.0000 - val_loss: 0.3535 - val_accuracy: 0.9500\n",
      "Epoch 306/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.2657e-05 - accuracy: 1.0000 - val_loss: 0.3545 - val_accuracy: 0.9500\n",
      "Epoch 307/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.3912e-05 - accuracy: 1.0000 - val_loss: 0.3550 - val_accuracy: 0.9500\n",
      "Epoch 308/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.0768e-05 - accuracy: 1.0000 - val_loss: 0.3562 - val_accuracy: 0.9500\n",
      "Epoch 309/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.1104e-05 - accuracy: 1.0000 - val_loss: 0.3555 - val_accuracy: 0.9500\n",
      "Epoch 310/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.8179e-05 - accuracy: 1.0000 - val_loss: 0.3570 - val_accuracy: 0.9500\n",
      "Epoch 311/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.6135e-05 - accuracy: 1.0000 - val_loss: 0.3579 - val_accuracy: 0.9500\n",
      "Epoch 312/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 7.5784e-05 - accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.9500\n",
      "Epoch 313/350\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 7.5063e-05 - accuracy: 1.0000 - val_loss: 0.3586 - val_accuracy: 0.9500\n",
      "Epoch 314/350\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 7.3761e-05 - accuracy: 1.0000 - val_loss: 0.3602 - val_accuracy: 0.9500\n",
      "Epoch 315/350\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 7.1626e-05 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9500\n",
      "Epoch 316/350\n",
      "23/23 [==============================] - 0s 9ms/step - loss: 7.0553e-05 - accuracy: 1.0000 - val_loss: 0.3611 - val_accuracy: 0.9500\n",
      "Epoch 317/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.1966e-05 - accuracy: 1.0000 - val_loss: 0.3614 - val_accuracy: 0.9500\n",
      "Epoch 318/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.9969e-05 - accuracy: 1.0000 - val_loss: 0.3620 - val_accuracy: 0.9500\n",
      "Epoch 319/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.8893e-05 - accuracy: 1.0000 - val_loss: 0.3628 - val_accuracy: 0.9500\n",
      "Epoch 320/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.9057e-05 - accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.9500\n",
      "Epoch 321/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.5537e-05 - accuracy: 1.0000 - val_loss: 0.3631 - val_accuracy: 0.9500\n",
      "Epoch 322/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.5111e-05 - accuracy: 1.0000 - val_loss: 0.3652 - val_accuracy: 0.9500\n",
      "Epoch 323/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.4769e-05 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9500\n",
      "Epoch 324/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.5857e-05 - accuracy: 1.0000 - val_loss: 0.3647 - val_accuracy: 0.9500\n",
      "Epoch 325/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.3572e-05 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 0.9500\n",
      "Epoch 326/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.9888e-05 - accuracy: 1.0000 - val_loss: 0.3673 - val_accuracy: 0.9500\n",
      "Epoch 327/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.7469e-05 - accuracy: 1.0000 - val_loss: 0.3667 - val_accuracy: 0.9500\n",
      "Epoch 328/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.1612e-05 - accuracy: 1.0000 - val_loss: 0.3695 - val_accuracy: 0.9500\n",
      "Epoch 329/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.9008e-05 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 0.9500\n",
      "Epoch 330/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.7360e-05 - accuracy: 1.0000 - val_loss: 0.3713 - val_accuracy: 0.9500\n",
      "Epoch 331/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.7682e-05 - accuracy: 1.0000 - val_loss: 0.3704 - val_accuracy: 0.9500\n",
      "Epoch 332/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.4096e-05 - accuracy: 1.0000 - val_loss: 0.3723 - val_accuracy: 0.9500\n",
      "Epoch 333/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.4371e-05 - accuracy: 1.0000 - val_loss: 0.3724 - val_accuracy: 0.9500\n",
      "Epoch 334/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.4460e-05 - accuracy: 1.0000 - val_loss: 0.3734 - val_accuracy: 0.9500\n",
      "Epoch 335/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.3910e-05 - accuracy: 1.0000 - val_loss: 0.3721 - val_accuracy: 0.9500\n",
      "Epoch 336/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.2443e-05 - accuracy: 1.0000 - val_loss: 0.3745 - val_accuracy: 0.9500\n",
      "Epoch 337/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.3918e-05 - accuracy: 1.0000 - val_loss: 0.3727 - val_accuracy: 0.9500\n",
      "Epoch 338/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.9542e-05 - accuracy: 1.0000 - val_loss: 0.3740 - val_accuracy: 0.9500\n",
      "Epoch 339/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.8862e-05 - accuracy: 1.0000 - val_loss: 0.3748 - val_accuracy: 0.9500\n",
      "Epoch 340/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.8823e-05 - accuracy: 1.0000 - val_loss: 0.3772 - val_accuracy: 0.9500\n",
      "Epoch 341/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.8604e-05 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9500\n",
      "Epoch 342/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.4605e-05 - accuracy: 1.0000 - val_loss: 0.3773 - val_accuracy: 0.9500\n",
      "Epoch 343/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.7241e-05 - accuracy: 1.0000 - val_loss: 0.3765 - val_accuracy: 0.9500\n",
      "Epoch 344/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4.5796e-05 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9500\n",
      "Epoch 345/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.5706e-05 - accuracy: 1.0000 - val_loss: 0.3804 - val_accuracy: 0.9500\n",
      "Epoch 346/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.3157e-05 - accuracy: 1.0000 - val_loss: 0.3800 - val_accuracy: 0.9500\n",
      "Epoch 347/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2797e-05 - accuracy: 1.0000 - val_loss: 0.3801 - val_accuracy: 0.9500\n",
      "Epoch 348/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2484e-05 - accuracy: 1.0000 - val_loss: 0.3811 - val_accuracy: 0.9500\n",
      "Epoch 349/350\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.2158e-05 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9500\n",
      "Epoch 350/350\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 4.1674e-05 - accuracy: 1.0000 - val_loss: 0.3826 - val_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train, validation_split=0.1,epochs=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "635a6c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24deaca0990>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5f585fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 5ms/step - loss: 1.1660 - accuracy: 0.9196\n"
     ]
    }
   ],
   "source": [
    "loss,accuracy=model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b9e82275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9195979833602905\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab53c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
